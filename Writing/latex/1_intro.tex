\section{Introduction}
	\pagenumbering{arabic}
	\subsection{Motivation}
		Social networks are playing an ever-growing role as an information source for a growing segment of the population. The influence of these networks is spreading out to new demographics, becoming more approachable to a variety of audiences of all ages and ranging from the tech savvy to technophobes. However, unlike traditional news outlets , social networks tend to offer anonymity to some extent to its users. This in turn, might eliminate the accountability, which traditional news outlets bear and is likely to be detrimental to the quality of information being spread. Another aspect of the content to be consumed is its relative and subjective interest to specific demographics. Different individuals are interested in different content and the ability to customize the materials presented to them should greatly increase both the user experience and the effectiveness of the news-delivery channel.
		
		\par
		
		The proposed study would examine such information exchanges in the form of messages (referred to hereinafter as \textit{posts}) in social networks while trying to classify the information to a category as well as evaluating its reliability. The purpose is to present a novel method to classify information to custom created labels based on a range of features of the content itself as well as  features of the author. The final product would be a classification of the post to a label  \{News; Not-News\} and a measurement of the reliability of such an assigned label could be ascertained on a numeric scale.
		
		\par
		
		The methodology of categorizing information to different labels corresponding to their usefulness was inspired by the work of \cite*{castillo2011information}. For the purpose of studying data reliability, Text Mining techniques as demonstrated by \cite*{go2009twitter} are to be explored. The field of data quality in social networks has not yet been throughly explored and novel methods of quantifying such quality, if found, could prove to be very useful. Especially in light of the growing dependence of different interest groups on said networks. 
		
		\par
		
		An interesting example of social network influence could be observed in the case when the official Twitter account of \textit{ The Associated Press} was hacked and a post saying that an attack on the white house occurred, and that president Obama was injured. Due to the automatization of many stock trading systems, the \textbf{S\&P500} index underwent a crash causing about 130B \$ to be wiped. The capital was mostly restored, however the magnitude of such fluctuations is not to be understated (\cite*{fake_post}). This study would provide a survey of the existing state of research as well as examine, compare and experiment with implementing the different proposed techniques on sample data from social networks. A comparative survey in statistical terms, could then be compiled.
		
		\par
		
		As a case study, the micro-blogging platform \textbf{Twitter} would be used. The platform offers an API which allows tapping into the Twitter's data stream and sending queries to their servers. These services are being offered free of charge to some extent. Namely, it is possible to tap to up-to 1\% of the real-time stream of data, which flows through Twitter. These should be more than sufficient for the spectrum of this study. Alternatively, several corpora of previously collected Twitter-Stream-Data are also available on the Internet. The bulk of research is to be conducted on  a specific topic, mainly popular trends in the branch of E-Commerce platforms such as Amazon, eBay or Otto. With an additional aspect being, broadening the scope and using tapping to a non-thematized data stream from Twitter (the twitter Steaming API allows for a non-filtered query).
		
	\subsection{Current State of Research}
		This study will observe how subjectively important content may be extracted from the Internet in general and from social networks in particular. This ongoing replacement of old-fashioned sources of information by on-line sources is occurring in numerous fields. Namely, \cite*{gaskins2012internet} explore how and to what extent the Internet replaces traditional media such as newspapers and television. They report that the replacement is not uniform across the board, and that some news sources remain overall not replaced by, but rather complemented by the Internet. This is true however, for a smaller fraction of the population, whereas the majority replace traditional news sources completely. This holds especially true for older news outlets, such as radio and printed press. Consequently, the importance of fact checking and setting up filters on social media grows, since Internet news are not subjected to professional scrutiny and review before being diffused.
		
		\par
		
		Due to the growing use and influence of Internet sources, the task of fact-checking and credibility assessment is of great importance. Generally, when discussing on-line news outlets, the issue of reputation comes into play, in the sense that some news websites enjoy higher credibility, when representing known and well respected organizations or due to establishing such reputation by practicing and upholding truthfulness and objectivity consistently. However, when observing social networks, where every user has the capability to spread (mis-)information, such credibility tend to be a rarity. \cite*{castillo2011information} studied this phenomena on social networks, taking the micro-blogging platform Twitter as a case study. They concluded that social media users tend to be quite receptive to content stemming from social media, especially when users are less experienced with Internet use. The research further explores the possibility of using machine learning methods to automatically assess credibility of Internet content with significant results. A Descriptive approach to feature building is undertaken. With this approach, data is initially processed and condensed before being passed into the algorithms for training. Such an approach will also be tested in this study. However, in the case of this study, the rigid definition of credibility is replaced with a more abstract measure, or \textit{subjective interest}.
		
		\par
		
		The concept of using machine learning techniques to provide quality content has been also explored on other platforms. Specifically, \cite*{covington2016deep} explored the function of Deep Neural Networks as a recommender system on Google's YouTube. The implemented algorithm learns through past experience which videos and categories are more likely to be favored by the user. To an extent, the users preferences are obviously subjective, which requires the recommender system to be trained and perpetually adjusted in a manner suiting each user personally. This is a classical example for a Neural Networks task. A core strength of such algorithms is learning classification schemes, which are difficult to put into strict rules, but appear intuitive to humans. Another strength of Deep Neural Networks is the ability to derive increasing marginal utility from the extremely large dataset, which Google possesses.
		
		\par
		
		\cite*{go2009twitter} also undertook a classification task with subjective or "noisy" labels, to borrow their terminology. They explored the use of non-basic elements of micro-blogging posts from Twitter (Tweets) such as Emoticons, words and details of the posting user for the purpose of classifying the \textit{sentiment} of a given post. The research studies different combinations of features-approaches and algorithms, which produce very comparable results, with no significant advantages for any one technique. The algorithms they train achieve around 80\% accuracy when classifying sentiment. These results show great promise and achieve significant forecasting power.
		
	\subsection{Possible Difficulties}
		Several problems might arise in the course of this study and will need to be addressed. Firstly, the procurement of data could prove to be inefficient since all the samples retrieved from the servers will have to be manually labeled. This process is likely to be very time consuming and limit the experimental data for this study. This difficulty is less likely to affect the practical implementation of any tools or methods produced by this work, since by that stage a validated and significant dataset would already be present. At the least, the very corpus, which I personally will build could be used to kick-start an automatic collection and labeling process.
		
		\par 
		
		Secondly, as mentioned priorly - the labels would be assigned by a single individual, myself, and according to my subjective perception of which observations qualify for the label \textit{news} and which do not. This in itself is not problematic, since the very goal is to mimic human interests. The difficulty arises in that only a single person's preferences are being learned. More robust results would be achieved, if the entire experiment would be repeated in several instances with different individuals, each labeling the data according to her own interests. The setting proposed for this study should provide a working example of the method, however repeating the experiment on various people would increase the credibility of the resulting technique. 
		
		\par 
		
		Lastly, several of the machine learning algorithms to be tested in the course of this work can be computationally expensive and as such might not scale well. For example, when constructing a \hyperref[svm]{Support Vector Machine} classifier, all data samples must be extrapolated as vectors into euclidean space $ \mathbb{R}^n $, with $ n $ representing the number of features. Following, the euclidean distance between each pair of vectors must be calculated. The calculation problem therefore grows in complexity exponentially. Nonetheless, such a scenario poses a problem only in the initial stage, since such complications are associated with the stage, in which classifiers are constructed. Once the classifier has been completed, the action of classification in itself should not require expensive computational resources.
		
	\subsection{Research Question}
		The foremost goal of the proposed research is answering the question of whether Machine Learning algorithms could be implemented as tools for predicting the subjective (\textit{fuzzy}) preferences of users, in regard to their consumption of news content, namely the subject matter which such users are being exposed to in social networks. To some extent an effort is evidently already being made by social network platforms to try and customize the substance and order of posts, which are presented to social network users. For example, such was the case when it was discovered that Facebook experimented on its users by changing how the content was presented to them (\cite{facebook_experiment}). Tweeter adjusts the users' preferences according to the people and organizations, which users actively choose to follow on the platform. In other words, this customization is done manually by the user, whereas it could prove beneficial to present some degree of automatization to the process and improve the system without the users' explicit effort.
	