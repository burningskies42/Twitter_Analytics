\section{Introduction}
	\pagenumbering{arabic}
	\subsection{Motivation}
		Social networks are playing an ever-growing role as an information source for a large public. The influence of said networks is spreading out to new demographics, becoming more approachable to a plethora of audiences of all ages and ranging from the tech savvy to technophobes. Unlike traditional news outlets however, social networks tend to offer anonymity to some extent to its users. This in turn, might eliminate the accountability, which traditional news outlets bear and is likely to be detrimental to the quality of information being spread.
		
		\par
		
		The proposed study would examine such information exchanges in the form of messages (referred to hereinafter as \textit{posts}) in social networks while trying to classify the information to a category as well as evaluating its reliability. The purpose is to present a novel method to classify information based on a range of features of the content itself as well as  features of the author. The final product would be a classification of the post to a label  \{News; Not-News\} and a measurement of the reliability of such an assigned label could be ascertained on a numeric scale.
		
		\par
		
		The methodology of categorizing information to different labels corresponding to their usefulness was inspired by the work of Castillo, Mendoza and Poblete (2001) \cite{castillo}. For the purpose of studying data-reliability, Text mining techniques as demonstrated by Go, Bhayani and Huang (2009)\cite{go} are to be explored. The field of data quality in social networks has not yet been throughly explored and novel methods of quantifying such quality, if found, could prove to be very useful. Especially in light of the growing dependen{\tiny }ce of different interest groups on said networks. An interesting example of social network influence could be obesrved in the case when the official Twitter account of \textit{ The Associated Press} was hacked and a post saying that an attack on the white house occured, and that president Obama was injured. Due to the automatization of many stock trading systems, the \textbf{ S\&P500} index underwent a crash causing about 130B \$ to be wiped. The capital was mostly restored, however the magnitude of such fluctuations ist not to be understated \cite{hack}. This study would provide a survey of the existing state of research as well as examine, compare and experiment with implementing the different proposed techniques on sample data from social networks. A comparative survey in statistical terms, could then be compiled.
		
		\par
		
		As a case study, the micro-blogging platform \textbf{Twitter} would be used. The platform offers an API which allows tapping into the Twitter's data stream and sending queries to their servers. These services are being offered free of charge to some extent. Namely, it is possible to tap to up-to 1\% of the real-time stream of data, which flows through Twitter. These should be more than sufficient for the spectrum of this study. Alternatively, several corpora of previously collected Twitter-Stream-Data are also available on the internet. The bulk of research is to be conducted on  a specific topic, mainly popular trends in the brach of E-Commrce platforms such as Amazon, eBay or Otto. With an additional aspect being, broadening the scope and using tapping to a non-thematized data stream from Twitter (the twitter Steaming API allows for a non-filtered query).
		\par
		
	\subsection{Current State of Research}
		In this study I observe how subjectively important content may be extracted from the Internet in general and from social networks in particular. This ongoing replacement of old-fashioned sources of information by on-line sources is occurring in numerous fields. Namely, Gaskin and Jerrit (2012) \cite{gaskins2012internet} explore how and to what extent the Internet replaces traditional media such as newspapers and television. They report that the replacement is not uniform across the board, and that some news source remain overall not replaced by but rather complemented by the Internet. This is true however, for a smaller fraction of the population, whereas the majority replace traditional news sources completely. This holds especially true for older sources, such as radio and printed press. Consequently, the importance of fact checking and setting up filters on social media grows, since Internet news are not subjected to professional scrutiny and review before being diffused.
		
		\par
		
		Due to the growing use and influence of Internet sources, the task of fact-checking and credibility assessment is of great importance. Generally, when discussing on-line news outlets, the issue of reputation comes into play, in the sense that some news websites enjoy higher credibility, when representing known and well respected organizations or due to establishing such reputation by practicing and upholding truthfulness and objectivity consistently. However, when observing social networks, where every user has the capability to spread information, such credibility tend to be a rarity. Castillo, Mendoza and Poblete (2011) \cite{castillo2011information} studied this phenomena on social networks, taking the micro-blogging platform Tweeter as a case study. They concluded that social media users tend to be quite receptive to Internet content, especially when users are less experienced with Internet use. The research further explores the possibility of using machine learning methods to automatically assess credibility of Internet content with significant results. A Descriptive approach to feature building is undertaken. With this approach, data is initially processed and condensed before being passed into the algorithms for training. Such an approach will also be tested in this study. However, in this case of this study the rigid definition of credibility is replaced with a more subjective measure, subjective interest.
		
		\par
		
		The concept of using machine learning techniques to provide quality content has been also explored on other platforms. Specifically, Covington, Adams and Sargin (2016) \cite{covington2016deep} explored the function of Deep Neural Networks as a recommender system on Google's YouTube. The algorithm learn through past experience which videos and categories are more likey to be favored by the user. To an extent, the users preferences are obviously subjective, which requires the recommender system to be trained and perpetually adjusted in a manner suiting each user personally. This is a classical example for a Neural Networks task. A core strength of such algorithms is learning classification schemes, which are difficult to put into strict rules, but appear intuitive to humans. Another strength of Deep Neural Networks is the ability to derive increasing marginal utility from the extremely large dataset, which Google possesses.
		
		\par
		
		Go, Bhayani and Huang 2009 \cite{go2009twitter} also undertook a classification task with subjective or "noisy" labels, to borrow their terminology. They explored the use of non-basic elements of micro-blog posts from Tweeter (Tweets) such as Emoticons, words and details of the posting user for the purpose of classifying the \textit{sentiment} of a given post. The research studies different combinations of features-approaches and algorithms, which produce very comparable results, with no significant advantages for any one technique. The algorithms they train achieve around 80\% accuracy when classifying sentiment. These results show great promise and achieve significant forecasting power.
	