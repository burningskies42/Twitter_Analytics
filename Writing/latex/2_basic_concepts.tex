\section{Basic Concepts}
	\subsection{Machine Learning}
		A sub-branch of computer science that rose to prominence and started evolving during the 1950s as part of research in the field of artificial intelligence. Machine learning refers the development of algorithms, which allow computers to learn from presented examples. The supposition being, the computer will thereafter be able to learn from its collected experience and automate the process of solving similar tasks. This process is coined by term \textit{training}.
		\par
		One definition of Machine - Learning, formulated by one of the fields pioneers, Tom M. Mitchell \cite{mitchell} "A computer program is said to learn from experience $E$ with respect to some class of tasks $ T $ and performance measure $ P $ if its performance at tasks in $ T $, as measured by $ P $, improves with experience $ E $."
		\par
		Nowadays machine learning algorithms are predominantly implemented as instruments for the analysis of real-world data in tasks, at which a concrete human-written application would prove ineffective. Such is the case for example with problems, which a person would be able to solve, but would not be able to determine the rules for solving explicitly. Or alternatively, where the rules are not constant, but rather evolve as time progresses. The purpose of teaching a machine to solve such tasks, probably relates to modeling, prediction or detection of details or certainties about the real world. 
		\par
		Vivid examples of real world uses are speech recognition as used in cellphones or in 
		call routing systems. Another common task for such systems is visual recognition of objects or characters. For such purposes, an algorithm is trained to recognize graphic patterns in images or diagrams. This practice is even implemented in medicine for recognizing malignant anomalies in X-ray or MRI scans. 
		
	\subsection{Text Mining}
		Text mining refers to the practice of extracting information from raw meta-data. Such data is obtained by surveying text corpora or other unstructured similar sources of verbatim data. The initial stage of processing consists of restructuring the data into a form compatible for statistical analysis. This includes but not limited to, segmenting the text to more basic building blocks such as paragraphs or sentences. This practice is known as \textbf{stemming}. Followed by cleaning up the data by removing non-informative words, which serve a grammatical role in human language, but tend to be of no use for language processing done by machines. In the next phase, words are normalized to their base \textbf{stem} by removing inflection modifying the word's tense, case number and other grammatical properties. In the professional nomenclature, this is referred to as \textbf{stemming} or \textbf{lemmatizing}.
		\par
		Methods used in text mining involve statistical pattern recognition, tagging-annotation and frequency analysis. The end goal of text mining is namely, the production of qualitative information out of raw text, often automatically, by using machine learning.
		\par
		Most of the usage of text mining methods in this work will be conducted using the \textbf{NLTK} module for the Python programming language. \textbf{NLTK} is a suite of libraries developed in the Department of Computer and Information Science at the University of Pennsylvania for Natural Language Processing \cite{nltk_book}.

	\subsection{Unstructured Data}
		Data which is derived from the Internet, namely social networks is oftentimes unstructured. Unstructured data refers to information not organized uniformly or in a predefined data model of sorts. Such a model should be composited harmoniously to the structure in which the data is to be eventually utilized. The aforementioned information is prone to heterogeneity in its composition and therefore varies in data types and may contain texts, number, dates as well as multimedia objects. Additionally, the data's integrity is susceptible to fluctuation, recurrently having missing or only partial data.
		\par
		In our case, the data comes in the form of Tweets originating from the Twitter servers. Tweets arrive packaged formatted as JSON objects. This type of construct has no standard morphology, and many of the Tweets' fields are subject to variation. The most common approaches to dealing with such data are either restructuring it to a new data-model or conducting a textual analysis aimed at recognizing patterns in its structure. Text Mining and Natural Language Processing are two prevailing mechanisms employed for this purpose. More on those in Chapter 3, \hyperref[sec:collectdata]{\textit{Collecting the Data}}.
		
	\subsection{Classification}
		The correct assignment of (often unstructured) data to a category (or class) out of a set of predetermined categories. In the case of unsupervised learning, creation of new categories which best segment the data and in case of unsupervised learning, assignment of new examples to user-defined classes. Supervised and Unsupervised Learning, are different approaches in Machine Learning, where the former requires continues input from the user whereas the latter is more autonomous. 
		
		\paragraph{Supervised Learning}
		Using this approach the algorithm is trained on a \textit{labeled training set}. This means that a list of numerous examples along with their features and the predetermined labeling, which denoted their class are fed as input into a learning algorithm. This algorithm than learns from the data, what common feature best characterize every given class. The different algorithms all take different approaches as to how better to split this observations. The finished machine is thereafter usually tested using novel data from the same source as the training data. The machine than measures its performance by trying to classify the testing data and comparing its classification to the actual labels. Some algorithms are even able to optimize themselves during the testing phase. Another common quality assurance test is called Cross-Validation. The data is split to $ m $ segments and trains itself in each round using $ m-1 $ segments and testing against the 1 remaining segment. In each round another segment is left out for testing. This measure assures the Machine's robustness.
		
		\paragraph{Unsupervised Learning}
		In the case of Unsupervised Learning, the task of classification is commonly similar to the task of \textit{Clustering}. During \textit{Clustering}, the Machine does not require the input (training-) data to be anteriorly \textbf{labeled}, or classified to predetermined categories. Instead, the \textit{Clustering} algorithm is usually given the number of categories (with some approaches, not even that). Next, the data is fractured according to what the Machine observes to be distinguishing features of the each category. Thus extracting and selecting features is of importance, since it further enhances the prediction power for future novel data. The features are often artificial constructs or rather functions transforming the original data from its original dimensions, in order to create distinguishing attributes of the input data. Examples of such features vary from binary presence indicators of a word or character, interaction variables of two or more features to transformations, linear or otherwise. some  common models in this field are Hierarchical Cluster Analysis (HCA), K-Means and Mean Shift.
		
	\subsection{Information Quality}
		Social Networks incorporate the interactions of millions of individuals, groups and organizations. A torrent of information of such dimensions easily qualifies for the qualification of Big Data. Social Networks data streams conform the the widely acknowledged 5 characteristics of Big Data, also knowns as the 5 \textbf{V}'s\cite{bigdata} standing for \textit{Volume}, \textit{Velocity}, \textit{Veracity}, \textit{Variety} and \textit{Value}. Of interest here is the data's correspondence between \textit{Veracity} and \textit{Value}, referring to the trustworthiness and statistical reliability of said facts, originating from a plethora of sources and presenting little to no accountability for its correctness. How much value can indeed be extracted from this data ?
		\par
		The questionable quality of such information makes basing critical business decisions on it, risky at best. One solution proposed to overcome these shortcomings would be to attach alongside the information quality metrics, which would describe its correctness, completeness and topicality as proposed by Klier and Heinrich(2016)[\cite{klier2016datenqualitat}].