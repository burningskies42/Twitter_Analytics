\section{Basic Concepts}
	\subsection{Machine Learning}
		A sub-branch of computer science that rose to prominence and started evolving during the 1950s as part of research in the field of artificial intelligence. Machine learning refers to the development of algorithms, which allow computers to learn from presented examples. The key assumption behind the concept being, that computers will thereafter be able to learn from their accumulated experience and automate the process of solving similar tasks. This process of machines learning from examples is coined by term \textit{training}.
		
		\par
		
		One definition of Machine Learning, formulated by one of the fields pioneers, Tom M. \cite{mitchell} "\textit{A computer program is said to learn from experience} E \textit{with respect to some class of tasks} T  \textit{and performance measure} P \textit{if its performance at tasks in} T , \textit{as measured by} P, \textit{improves with experience} E".
		
		\par
		
		Nowadays machine learning algorithms are predominantly implemented as instruments for the analysis of real-world data at tasks, which an explicit human-written algorithm would prove ineffective. For example, such is the case with problems, which a person would be able to solve, but would not be able to delineate clearly the rules for solving explicitly. Or alternatively, where the rules are not constant, but rather evolve as time progresses. The purpose of teaching a machine to solve such tasks, customarily relates to modeling, prediction or detection of details or certainties about real world phenomena. 
		
		\par
		
		A noteworthy example of contemporary implementations of Machine Learning is speech recognition. Cellphones and call routing systems make extensive use of Machine Learning to convert human voice into text and code. Another common Machine Learning task is visual recognition of objects or characters. For such purposes, an algorithm is trained to recognize graphic patterns in images or diagrams. This practice is making breakthrough progress in medicine, namely recognizing malignant anomalies in X-ray or MRI scans. 
		
	\subsection{Text Mining}
		Text Mining refers to the practice of extracting information from raw verbal corpora. Such data is obtained by surveying text bodies or other similarly unstructured sources of verbatim data. The initial stage of processing consists of restructuring the data into a form compatible for statistical analysis. This commonly includes but not limited to, segmenting the text into more basic building blocks, such as paragraphs, sentences or even single words. This practice is called \textbf{chunking} for phrases and \textbf{chinking} for breaking up \textit{chunks} into words. The next stage consists of cleaning up the data by removing non-informative words, which usually serve a grammatical role and are therefore crucial in human language, but tend to be of no use for language processing done by machines. Thereupon, the remaining words are normalized to their base \textbf{stem} by removing inflections modifying the word's tense, case number and other grammatical properties. In the professional nomenclature, this is referred to as \textbf{stemming} or \textbf{lemmatizing}.
		\par
		These preprocessing methods, which are used in text mining involve statistical pattern recognition, tagging-annotation and frequency analysis. The end goal of text mining is namely, the production of qualitative information out of raw text, often automatically, by using Machine Learning.
		\par
		Most of the usage of Text Mining methods in this work will be conducted using the \textbf{NLTK} module for the Python programming language. \textbf{NLTK} is a suite of libraries developed for Natural Language Processing in the Department of Computer and Information Science at the University of Pennsylvania (\cite{nltk_book}).

	\subsection{Unstructured Data}
		Data which is derived from the Internet, specifically social networks is oftentimes unstructured. Unstructured data refers to information not organized uniformly or in a predefined data model of sorts. Such a model should be composited harmoniously to the structure in which the data is to be eventually utilized. The aforementioned information is prone to heterogeneity in its composition and as consequence, varies in data types and may contain texts, number, dates as well as multimedia objects simultaneously. Additionally, the data's integrity is susceptible to fluctuation, recurrently having missing or only partial data.
		
		\par
		
		In our case, the data comes in the form of Tweets originating from the Twitter servers. Tweets arrive packaged and formatted as JSON (JavaScript Object Notation) objects. This type of construct has no standard morphology, and many of the Tweet's fields are subject to variation. The most common approaches to dealing with such data are either restructuring it to a new data-model or conducting a textual analysis aimed at recognizing patterns in its structure. Text Mining and Natural Language Processing are two prevailing mechanisms employed for this purpose. More on those in Chapter 4, \hyperref[sec:collect_data]{\textit{Data Collection}}.
		
	\subsection{Classification}
		The task of correct assigning (often unstructured) data to a category (or class) from a set of predetermined categories. In the case of Unsupervised Learning, creation of new categories which best segment the data and in case of Supervised Learning, assignment of new examples to user-defined classes. Supervised and Unsupervised Learning, are different approaches to Machine Learning programming, where the former requires continuous input from the user, whereas the latter is autonomous. 
		
		\paragraph{Supervised Learning}
			Using this approach, the algorithm is trained on a \textit{labeled training set}. This means that a list of numerous examples along with their features and the predetermined labels, denoting their respective classes are fed as input into a learning algorithm. This algorithm than analyzes the data, and extracts common features, which best characterize every given class. The different algorithms all apply distinct approaches as to how better split these observations. The finished Machine is thereafter usually tested using novel data from the same source as the training data. The performance of the Machine is measured  by classifying the testing data and comparing its classification to the actual labels. Some algorithms are even able to optimize themselves during the testing phase. Another common quality assurance test is called Cross-Validation. During Cross-Validation, the data is split to $ m $ segments and trains itself in each round using $ m-1 $ segments and testing against the 1 remaining segment. In each round, a different segment is left out for testing. This measure assures the Machine's robustness.
		
		\paragraph{Unsupervised Learning}
			In the case of Unsupervised Learning, the task of classification is commonly similar to the task of \textit{Clustering}. During \textit{Clustering}, the Machine does not require the input (training-) data to be anteriorly \textbf{labeled}, or classified to predetermined categories. Instead, the \textit{Clustering} algorithm is usually given the number of categories (with some approaches, not even that). Next, the data is fractured according to what the Machine observes to be distinguishing features of the each category. Thus extracting and selecting features of importance, since it further enhances the prediction power for future novel data. The features are often artificial constructs or rather functions transforming the original data from its native dimensions, in order to create distinguishing attributes of the input data. Examples of such features vary from binary presence indicators of a word or character, interaction variables of two or more features to transformations, linear or otherwise. Some common models in this field are Hierarchical Cluster Analysis (HCA), K-Means and Mean Shift.
		
	\subsection{Information Quality}
		Social Networks incorporate the interactions of millions of individuals, groups and organizations. A torrent of information of such proportions easily qualifies for the qualification of Big Data. Social Networks data streams conform the the widely acknowledged 5 characteristics of Big Data, also knowns as the 5 \textbf{V}'s (as in \cite{bigdata}) standing for \textit{Volume}, \textit{Velocity}, \textit{Veracity}, \textit{Variety} and \textit{Value}. Of interest here is the data's correspondence between \textit{Veracity} and \textit{Value}, referring to the trustworthiness and statistical reliability of said facts, originating from a plethora of sources and presenting little to no accountability for its correctness and relevancy. How much value can indeed be extracted from this data ?
		
		\par
		
		The questionable quality of such information makes basing critical business decisions on it, risky at best. One solution proposed to overcome these shortcomings would be to attach alongside the information quality metrics, which would describe its correctness, completeness and topicality as proposed by \cite{klier2016datenqualitat}.