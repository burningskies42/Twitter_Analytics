\section{Methodology}
	The approach undertaken in this study is similar to previous works in the field of applied Machine Learning. Initially, the main theme of the is selected according to its quality and volume. In the case of this research, the main point of inquiry I wish to study regards Tweets relating to the subject of E-Commerce platforms. Such platforms usually see an abundance of Tweets fulfilling the volume requirement. However, additional steps are taken to insure the informations quality. These cleaning measures are elaborated in the \hyperref[Application]{Application chapter}.
	\par
	On of the main motives of this research is to observe whether social media data can be evaluated in real-time and distilled into practical knowledge. With this consideration in mind, the data used in this work should bear as much resemblance as possible to a live Tweeter data stream. Acknowledging this consideration, the raw data in the form of Tweets in gathered from the Twitter databases. Namely, the data is collected synchronously, as it is intercepted by the servers. The \textit{Streaming} Application Programming Interface (API)\cite{stream_api} is implemented for such use-cases. Employing the \textit{Streaming} API, a connection is established to the servers, which captures a narrow stream (about 15\%) of all Tweets relevant to a given search term. The reduction of stream is due to the complexity in both sending and receiving such a volume of data. Twitter also offers a full non-reduced access through the \textit{Firehose} API.
	\par
	The captured data, must then be restructured to fit the data-model of this study, specifically input appropriated for Machine Learning purposes. This includes a preliminary analysis of the data and removal of incomplete observations. The JSON data structure in which Tweets are stored allows for a dynamic non-standard structure, which in turn translates to non-standardized data. Therefore some Tweets must be adapted to conform to a unitary pattern. It is also common for Tweets to retrospectively be removed from the Tweeter servers, either by their owners or by Twitter moderators. Such Tweets cannot be analyzed, since they are no longer available on-line. The process of gathering data and cleansing it is discussed in \hyperref[sec:collect_data]{Part [4.1]}.
	\par
	The next stage entails restructuring the raw captured Tweets into datasets. For this purpose, \textit{features}\footnote{\label{ml_note}Machine Learning nomenclature} are extracted from each observation and converted to a standard array, representing the original Tweet. \textit{Features} are unique properties of the data, which are describe it and also its owner, dependent on the approach. Different types of \textit{Features} are used in accordance with the Learning approach undertaken. These approaches are further discussed in \hyperref[build_features]{Part [4.2]}.
	\par
	These features are then analyzed for consistency and correctness. Subsequently, the features are passed into different Machine Learning algorithms, in order to \textit{train}\footnotemark[1] them. \textit{Training} is the process of deducing the decision rules for classifying the data into one of the categories. This deduction is based on the information the algorithm draws from the input data. Afterwards, the empirical success of these different algorithms will be statistically measured and summarized. Additionally, implications are to be drawn about other use-cases. The classification task observed here is subjective in  nature, since the classification themselves are abstract and arguable. Success in this experiment could prove that similarly subjective classification projects are practical. 
	
	\subsection{Training Classifier}
	The purpose of \textit{training} is to create Classifiers. A Classifier is a function which is fed  new observations and automatically recognizes and \textit{labels} them. The intrinsic decision algorithm, through which the Classifier will decide how to allocate a novel observation, usually remains hidden and operates as a black-box of sorts. Seeing that the decision rules could be numerous and considerably not intuitive for human readers, they remain hidden. Particularly so, when said algorithms are convolutional. Convolutional Machine Learning schemes, such as Deep Neural Networks, may incorporate numerous stages of parameter construction which renders them practically incomprehensible to human users. The actual implementation of all the algorithms would be programmed in the Python programming language and will primarily make use of the scikit-learn\cite{scikit-learn} module.
	\par
	The data used for the purpose of this study consolidates 12.520 unique Tweets. Different approaches vary in the percentage of the data used from this corpus. In order to insure robust results, the \textit{Hold-out Method} is used across the board. For each instance of \textit{training} the data is split into two parts - a \textit{training} set and testing set. The \textit{training set} would usually be allocated the larger portion of the data (between 60\% and 90\%) and would be used, as the name suggests for training the Classifiers. The remaining corpus chunk (between 10\% and 40\%) would be used for testing the Classifiers success. During each such training-testing session, the data corpus is shuffled. This in turn means, that the training and testing sets constantly differ. This process of splitting, training and testing using different data in each iteration should produce statistically significant results. This method of constantly splitting the data randomly ensures the results robustness.
	\par
	The following paragraphs expand on the different Machine Learning schemes. Figure 3 illustrates these schemes with relation to computational complexity and their interoperability.
	
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.8\textwidth]{methods}
		\captionsetup{width=0.8\textwidth}
		\caption{Clustering of Different Classifiers}
	\end{figure}
	
	\subsection{Supervised Learning}
	
	\input{regressions}
	\input{naive_bayes}
	\input{svm}
	\input{ann}
	\input{trees}

	
	
	
	
	
	
	
	
	
	
	
	
		
	