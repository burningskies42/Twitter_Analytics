\section{Methodology}
	The approach I undertake in this study is similar to previous works in the field. Initially, data is gathered on a given theme. In the case of my research, the main query I wish to study would regard Tweets relating to the subject of E-Commerce platforms.
	\par
	On of the motives of this research is to observe whether information originating from social media can be evaluated in real-time. With this consideration in mind, the data should bear as much resemblance to a live information torrent on Tweeter as possible. Acknowledging this consideration, the raw data in gathered in the form of Tweets originating from the Twitter databases. I collect the data synchronously, as it is intercepted by the servers. The \textit{Streaming} Application Programming Interface (API)\cite{stream_api} is usually implemented in such use-cases. Using the \textit{Streaming} API, a connection is established to the servers, which captures a narrow stream (about 15\%) of all Tweets relevant to a given search term. 
	\par
	The captured data, must then be restructured to fit the data-model of this study, namely appropriate input for Machine Learning algorithms. This includes a preliminary analysis of the data and removal of incomplete observations. The data structure in which Tweets are stored allow for a dynamic non-standard structure, which in turn means they vary in properties. Therefore some Tweets must be adapted to conform to a unitary pattern. It is also quite common for Tweets to retrospectively be removed, either by themselves or by Twitter moderators. Such Tweets cannot be analyzed, since they are no longer available on-line. The process of gathering data and cleansing it is discussed in \hyperref[sec:collect_data]{Part [4.1]}.
	\par
	The next stage entails representing the data in a form which is suitable for Machine Learning Algorithms. For this purpose, I build \textit{features}\footnote{\label{ml_note}Machine Learning nomenclature} for each observation. \textit{Features} are unique properties of the data, which are used to describe it. Different types of \textit{Features} are used in accordance with the Learning approach undertaken. These approaches  we discuss in \hyperref[build_features]{Part [4.2]}.
	\par
	These features are then analyzed for consistency and correctness. Subsequently, the features are passed into different Machine Learning algorithms, in order to \textit{train}\footnotemark[1] them. \textit{Training} is the process of deducing the decision rules for classifying the data into one of the categories. This deduction is based on the information the algorithm draws from the input data. Afterwards, the empirical success of these different algorithms will be statistically measured the summarized. Additionally, implications are to be drawn about other use-cases, which are out of the scope of this research. 
	

	
	