\section{Methodology}
	The approach I undertake in this study is similar to previous works in the field. Initially, data is gathered on a given theme. In the case of my research, the main query I wish to study would regard Tweets relating to the subject of E-Commerce platforms.
	\par
	On of the motives of this research is to observe whether information originating from social media can be evaluated in real-time. With this consideration in mind, the data should bear as much resemblance to a live information torrent on Tweeter as possible. Acknowledging this consideration, the raw data in gathered in the form of Tweets originating from the Twitter databases. I collect the data synchronously, as it is intercepted by the servers. The \textit{Streaming} Application Programming Interface (API)\cite{stream_api} is usually implemented in such use-cases. Using the \textit{Streaming} API, a connection is established to the servers, which captures a narrow stream (about 15\%) of all Tweets relevant to a given search term. 
	\par
	The captured data, must then be restructured to fit the data-model of this study, namely appropriate input for Machine Learning algorithms. This includes a preliminary analysis of the data and removal of incomplete observations. The data structure in which Tweets are stored allow for a dynamic non-standard structure, which in turn means they vary in properties. Therefore some Tweets must be adapted to conform to a unitary pattern. It is also quite common for Tweets to retrospectively be removed, either by themselves or by Twitter moderators. Such Tweets cannot be analyzed, since they are no longer available on-line. The process of gathering data and cleansing it is discussed in \hyperref[sec:collect_data]{Part [4.1]}.
	\par
	The next stage entails representing the data in a form which is suitable for Machine Learning Algorithms. For this purpose, I build \textit{features}\footnote{\label{ml_note}Machine Learning nomenclature} for each observation. \textit{Features} are unique properties of the data, which are used to describe it. Different types of \textit{Features} are used in accordance with the Learning approach undertaken. These approaches  we discuss in \hyperref[build_features]{Part [4.2]}.
	\par
	These features are then analyzed for consistency and correctness. Subsequently, the features are passed into different Machine Learning algorithms, in order to \textit{train}\footnotemark[1] them. \textit{Training} is the process of deducing the decision rules for classifying the data into one of the categories. This deduction is based on the information the algorithm draws from the input data. Afterwards, the empirical success of these different algorithms will be statistically measured the summarized. Additionally, implications are to be drawn about other use-cases, which are out of the scope of this research. 
	
	\subsection{Training Classifier}
	The purpose of \textit{training} is to create Classifiers, which automatically recognize and \textit{label} new observations. The intrinsic	decision algorithm, through which the Classifier will decide how to classify a novel observation, usually remain hidden and operate as a black-box of sorts. Since the decision rules could be numerous and far from intuitive for human readers. This is especially the case, when said algorithms are convolutional. Convolutional machine learning schemes, such as Deep Neural Networks, may incorporate numerous stages of parameter construction which renders them practically incomprehensible to human users. The actual implementation of all the algorithms would be programmed in the Python programming language and will be primarily making use of the scikit-learn\cite{scikit-learn} module.
	\par
	The data used for the purpose of this study consolidates 12.520 unique Tweets. For the purpose of reaching robust results, the \textit{Hold-out Method} is used as follows. For each instance of \textit{Algorithm training} the entire data corpus is split into two parts - a training set and testing set. The training set would usually be allocated the larger portion of the data (about 70\%) and would be used, as the name suggests for training the Classifiers. The rest of the data (about 30\%) would be used for testing the Classifiers accuracy. During each such training-testing session, the data corpus is shuffled. This in turn means, that the training and testing sets constantly differ. This process of splitting, training and testing using different data in each iteration should produce statistically significant results. This method of constantly splitting the data randomly ensures the results robustness.
	\par
	The following paragraphs expand on the different Machine Learning schemes. Figure 3 illustrates these schemes according to their types.
	
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.8\textwidth]{methods}
		\captionsetup{width=0.8\textwidth}
		\caption{Clustering of Different Classifiers}
	\end{figure}
	
	\subsection{Supervised Learning}
	
	\subsubsection{Regressions}
	One approach and probably the most basic one is simply running a regression [\ref{regression}] with all the features as the independent variables and the a numeric representation of the classes as the dependent variable. Some threshold value for the classes must be determined since the estimate for the explained variable, will have a non-deterministic value. 
	
	\begin{equation}
		\mathbb{E}(\hat{y} \ \vert \ \textbf{x}) \approx 
		\begin{cases}
			label = \text{News}, &\hat{y} \geq y^* \\
			label = \text{Not News},\ \ &\hat{y} < y^*
		\end{cases}
		\label{regression}
	\end{equation}
	
	\par
	\paragraph{Linear Regression} 
		This method builds a linear dependency system between the explained variable (the \textit{Class} in this case) and the explaining variables (\textit{features}). With the \textit{Bag-of-Words} approach, each variable \textit{x} is a dummy variable indicating whether a given word \textit{m} is present in Tweet \textit{i}. The estimations parameters, which are denoted with $\beta_j$ are derived using Ordinary Least Squares. In itself, the linear regression estimation is a weak predictor for the purpose of classification, but it allows for calculating other useful statistics such as the coefficient of determination, commonly known as $R^2$. This static demonstrates the part of the variance that is explained using the provided variables and is also sometimes called 'goodness of fit'. We therefore strive to maximize it as much as possible. The more of the variance that is covered by our variables, the better can we predict the out come of the classification. Another point of interest is the distribution of predicted classifications ($\hat{y}$). In an ideal scenario, the distribution of $\hat{y}$ would be concentrated around the numerical representations of the two classed, as follows:
	
	The basic premise for the use of linear regressions as classification tools, where n is the number of observations, m is the number of features [\ref{lin_reg}].
	
	\begin{equation}
		y_i = \beta_1 x_{1i}+ \beta_2 x_{2i} + ... + \beta_m x_{mi} + \epsilon_i \ \ \ \ \ \ \ \ 
		\forall \ i \in [1,n].
		\label{lin_reg}
	\end{equation}
	
	\paragraph{Logistic Regression} Despite its name, the actual regression model executed here is linear, and is used primarily for classification rather than regression analysis. This regression scheme differs from linear firstly, in the fact, that the possible outcomes of the dependent variable are discreet rather than continuous. Secondly, the probabilities which describe the different outcomes of a regression instance, are modeled using a logistic function. The discrete outcomes can in turn be converted to labels, which in allows for a much more fluent application as a classification tool and is therefore commonly employed in  Machine Learning. Logistic regression analysis is formally represented as follows in  [\ref{logit}]. The $x$ vector represents all explanatory variables, in our case, the Tweet features. The term error $\epsilon$ follows the standard logistic distribution, which also gives the regression its name. 
	
	\begin{equation}
	y = 
	\begin{cases}
	1 \ \  \beta_0 + \beta_1 x + \epsilon > 0 \\
	0 \ \  \text{else } 
	\end{cases} \text{         } \forall \  x = 
	\begin{pmatrix}x_1\\x_2\\...\\x_n \end{pmatrix}
	\label{logit}
	\end{equation}
	
	\input{svm}
	
	\subsubsection{Artificial Neural Networks}
		An Artificial Neural Network (ANN) \cite{mcculloch1943logical} is a mathematical computational model, whose development was inspired by cognitional processes in the brains' nervous system. This sort of network typically consists of numerous interconnected input and output information units. The form of connectivity between these units embodies the connection strength, in a fashion similar to how neurons are linked in the brain. ANNs are predominantly used in Cognitive Sciences and related software such as Artificial Intelligence Systems. Common tasks for such systems are character recognition, facial recognition, financial markets prediction and test mining among other uses.
		
		\paragraph{Intuition}
		A neural network is composed of linked processors called neurons, each capable of executing a simple mathematical operation. However, when combined said networks are capable of sophisticated problem solving. Any given neuron receives inputs $ i $ with according weights $ w $. A sum of all weighted inputs is then calculated. When this sum exceeds a threshold value $ T $ the output $ O $ is equal to one and zero otherwise as in [\ref{ann_neuron_sumprod}]. This operation resembles the operation of a biological neuron, which releases an electric signal when its agitation transcends a certain limit.
		
		\begin{equation}
			\langle i,w \rangle = \sum_j i_j w_j = 
				\begin{cases}
					1 \ \ \text{,  } \sum_j i_j w_j \geq T \\
					0 \ \ \text{,  } \sum_j i_j w_j < T 
				\end{cases}
				 = O
			\label{ann_neuron_sumprod}
		\end{equation}
		
		\begin{figure}[h]
			\centering
			\captionsetup{width=0.8\textwidth}
			\includegraphics[width=0.8\textwidth]{ANN_percept.png}
			\caption[ANN Perceptron]{
				\footnotesize{
					Structure of a Perceptron with $ n $ input nodes $ i $ and output $ o $. Output 1 when threshold is surpassed and zero otherwise.
				}
			} 
			\label{ANN_percept}
		\end{figure}
		
		The organization of neurons, also called Perceptrons, is crucial to the networks operation and the the input weights are the ones, which determine the pattern to be recognized. Therefore the prime objective when constructing an ANN is determining proper values for the weights. The neurons are then used as logical gateway, which can carry out the "AND" and "OR" operations. A major obstacle in the early days of ANNs was that it was demonstrated that a logical gate of the type "XOR" (exclusive OR) could not be achieved irregardless of the weights on the inputs. This difficulty was later overcome with the introduction of multi-layered ANN.
		
		Multi-layered ANNs are build from several layers of neurons, where all layers except the first(input layer) and the last (output layer) receive their inputs from the lower level of neurons and output into the higher layer of neurons, which in turn use this output as their input. It was than demonstrated that a "XOR" gateway could be constructed using a 3-layered network [\ref{ANN_XOR}].
		
		\begin{figure}[h]
			\centering
			\captionsetup{width=0.8\textwidth}
			\includegraphics[width=0.8\textwidth]{ANN_XOR.png}
			\caption[ANN XOR Perceptron-Network]{
				\footnotesize{
					A three layer Network, creating a XOR gateway.
				}
			} 
			\label{ANN_XOR}
		\end{figure}
		
	
	 \cite{bishop1995neural}
	\subsection*{Stochastic Gradient Descent}
	{\color{red} placeholder}
	\subsection*{Nearest Neighbors}
	{\color{red} placeholder} \cite{bay1998combining}
	\subsection*{Naive Bayes}
	{\color{red} placeholder} \cite{rish2001empirical}
	\subsection*{Decision Trees and Ensemble Methods}
	{\color{red} placeholder}
	\cite{quinlan2014c4} \cite{breiman1984classification}
	\subsection*{Stochastic Gradient Descent}
	{\color{red} placeholder}
	
	\newpage
	\subsection{Unsupervised Learning}
	{\color{red} placeholder}
	\subsubsection*{Gaussian Mixture}
	{\color{red} placeholder}
	\subsubsection*{Clustering}
	{\color{red} placeholder}
	\subsubsection*{Covariance estimation}
	{\color{red} placeholder}
	\subsubsection*{Neural network models (unsupervised)}
	{\color{red} placeholder}
	
	
	
	
	
	
	
	
	
	
	
	
		
	