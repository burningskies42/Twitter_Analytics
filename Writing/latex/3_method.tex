\section{Methodology}
	The approach I undertake in this study is similar to previous works in the field. Initially, data is gathered on a given theme. In the case of my research, the main query I wish to study would regard Tweets relating to the subject of E-Commerce platforms.
	\par
	On of the motives of this research is to observe whether information originating from social media can be evaluated in real-time. With this consideration in mind, the data should bear as much resemblance to a live information torrent on Tweeter as possible. Acknowledging this consideration, the raw data in gathered in the form of Tweets originating from the Twitter databases. I collect the data synchronously, as it is intercepted by the servers. The \textit{Streaming} Application Programming Interface (API)\cite{stream_api} is usually implemented in such use-cases. Using the \textit{Streaming} API, a connection is established to the servers, which captures a narrow stream (about 15\%) of all Tweets relevant to a given search term. 
	\par
	The captured data, must then be restructured to fit the data-model of this study, namely appropriate input for Machine Learning algorithms. This includes a preliminary analysis of the data and removal of incomplete observations. The data structure in which Tweets are stored allow for a dynamic non-standard structure, which in turn means they vary in properties. Therefore some Tweets must be adapted to conform to a unitary pattern. It is also quite common for Tweets to retrospectively be removed, either by themselves or by Twitter moderators. Such Tweets cannot be analyzed, since they are no longer available on-line. The process of gathering data and cleansing it is discussed in \hyperref[sec:collect_data]{Part [4.1]}.
	\par
	The next stage entails representing the data in a form which is suitable for Machine Learning Algorithms. For this purpose, I build \textit{features}\footnote{\label{ml_note}Machine Learning nomenclature} for each observation. \textit{Features} are unique properties of the data, which are used to describe it. Different types of \textit{Features} are used in accordance with the Learning approach undertaken. These approaches  we discuss in \hyperref[build_features]{Part [4.2]}.
	\par
	These features are then analyzed for consistency and correctness. Subsequently, the features are passed into different Machine Learning algorithms, in order to \textit{train}\footnotemark[1] them. \textit{Training} is the process of deducing the decision rules for classifying the data into one of the categories. This deduction is based on the information the algorithm draws from the input data. Afterwards, the empirical success of these different algorithms will be statistically measured the summarized. Additionally, implications are to be drawn about other use-cases, which are out of the scope of this research. 
	
	\subsection{Training Classifier}
	The purpose of \textit{training} is to create Classifiers, which automatically recognize and \textit{label} new observations. The intrinsic	decision algorithm, through which the Classifier will decide how to classify a novel observation, usually remain hidden and operate as a black-box of sorts. Since the decision rules could be numerous and far from intuitive for human readers. This is especially the case, when said algorithms are convolutional. Convolutional machine learning schemes, such as Deep Neural Networks, may incorporate numerous stages of parameter construction which renders them practically incomprehensible to human users. The actual implementation of all the algorithms would be programmed in the Python programming language and will be primarily making use of the scikit-learn\cite{scikit-learn} module.
	\par
	The data used for the purpose of this study consolidates 12.520 unique Tweets. For the purpose of reaching robust results, the \textit{Hold-out Method} is used as follows. For each instance of \textit{Algorithm training} the entire data corpus is split into two parts - a training set and testing set. The training set would usually be allocated the larger portion of the data (about 70\%) and would be used, as the name suggests for training the Classifiers. The rest of the data (about 30\%) would be used for testing the Classifiers accuracy. During each such training-testing session, the data corpus is shuffled. This in turn means, that the training and testing sets constantly differ. This process of splitting, training and testing using different data in each iteration should produce statistically significant results. This method of constantly splitting the data randomly ensures the results robustness.
	\par
	The following paragraphs expand on the different Machine Learning schemes. Figure 3 illustrates these schemes according to their types.
	
	\begin{figure}[h]
		\centering
		\includegraphics[width=0.8\textwidth]{methods}
		\captionsetup{width=0.8\textwidth}
		\caption{Clustering of Different Classifiers}
	\end{figure}
	
	\subsection{Supervised Learning}
	
	\input{regressions}
	\input{naive_bayes}
	\input{svm}
	\input{ann}
	\input{trees}

	
	
	
	
	
	
	
	
	
	
	
	
		
	