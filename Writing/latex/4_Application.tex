\section{Application}
\label{Application}
	The outline of implementing the technique described in the previous chapter is as follows. Initially, a text corpus made out of a collection of tweets in queried from the Twitter databases. The Tweets are requisitioned according to a word parameter. All Tweets containing this parameter at the time of capture will be stored as part of the dataset. Next, the data will be cleaned of duplicates and possible entries made by bots. The cleaned data would then be labeled manually using a custom made Python application. The labeled data can be now used to construct features representing the data. Features are constructed in a fashion, which allows them to be passed as input data into Machine Learning algorithms. The process of constructing classifiers based of features is named \textit{training}. The completed classifiers can be further tweaked to improve their accuracy. Cross Validation is conducted to measure the quality of the complete classifiers. Finally, the ready classifiers can be deployed to classify data in real time.

	\subsection{Information Procurement}
	The main theme of the collected data would concentrate around the topic of Internet Sales Platforms also known as E-Commerce. Several aspects make the topic beneficial for this research. Firstly,  such companies are almost exclusively web-based. It is therefore probable, that most of the company's marketing efforts as well as overall news relating to such a firm, would circulate first and foremost in the Internet. Secondly, all company-relevant news are attainable from the web before all other resources. Hence, such news will in all likelihood seep to social media faster than announcements and other stories, which are predominantly covered by traditional media such as television and Newspapers. Therefore, the E-Commerce theme is likely to be widely and swiftly covered on social media, which is beneficial for the purposes of this study, since social media is used as the source of all data.
	
		\subsubsection*{Search Terms}
			The data was collected in the form of relevant Tweets from the Twitter Stream API. A Tweet would be considered relevant if it contained a search parameter related contextually to E-Commerce. The main efforts were concentrated around the web-store Amazon. \textbf{Amazon} appears to the most fruitful search parameter, in terms of the quantity of Tweets relating to it. Additional search words that were tested were {\bfseries Alibaba, Zalando} and {\bfseries Groupon}, but proved to be impractical due to less data. 
			
			\par
			
			The widespread mention of Amazon in Tweets is somewhat over-inflated due to the extensive use of Amazon gift cards. Amazon gift cards have become preponderate owing to their variety of uses. A few examples of common practices involving Amazon gift cards are rewarding users for services, such as polls and questionnaires, enticing people to take part in events or groups, and being offered as general rewards in competitions and games. The plethora of uses, facilitates Amazon gift cards to be viewed as a sort of pseudo-currency in the Internet. In turn this means, that Amazon could be mentioned in a Tweet, despite the context only indicating the Gift card and being completely unassociated to the E-Commerce platform directly. Such Tweets are generally less valuable for the purpose of gaining knowledge about Amazon itself.
		
		\subsubsection*{Collecting the Data}
		\label{sec:collect_data}
			The gathering of Tweets was executed using a program written in the Python programming language for the purpose of this research. All software created for this project is available on \href{https://github.com/burningskies42/Twitter_Analytics}{\textit{GitHub}} \footnote{\url{https://github.com/burningskies42/Twitter_Analytics}}. A wrapper module called \textit{Tweepy}\footnote{\url{http://tweepy.readthedocs.io/en/v3.5.0/}} was implemented for interacting with the Twitter Streaming API. Tweepy is an open source interface, which allows communicating with the Twitters servers and sending queries requesting specific information from Twitter's databases. The interface allows for two main type of queries, \textit{Rest} and \textit{Streaming}. The former scans the servers for information posted on Twitter in the past whereas the latter, as the name implies, connects to an active data stream containing a narrowed down flow of Tweets being actively published by Twitter users. Both types of API's are being offered for free to a certain extent, whereas almost unrestricted versions of the same API are offered as a proprietary fee-based products of Twitter. The free version of the REST API is restricted to looking up Tweets posted in the last two to three weeks. Similarly, the gratis version of the Streaming API is restricted to a fire-hose narrowed down to about 15\% of the total Bandwidth of all current Tweets.
			
			\par
			
			The data units incoming from the Twitter servers are sent as JavaScript Object Notation (JSON) strings. JSONs are structured hierarchically, which allows for embedding other JSON objects in them recursively. This makes JSON beneficial for Tweets, since it allows multi-level storage of Tweet propertries and objects. For example, one of the JSON objects integrated in each Tweet JSON is the USER object for the Tweet-poster. A USER object, a JSON by itself, contains all relevant data for a Tweeter account. The USER object in turn contains all data publicly available in Twitter about a Twitter account such as, location, date of registration, homepage etc. An additional object of interest is the ENTITIES JSON object, which contains all outside references from the Tweet's text such as, URLs, Multimedia, References to other Tweets or other users. 
			This structure greatly eases the analysis of a Tweet and its features, since most of the necessary data is available from the Tweet itself and no further queries about the Tweets and its posting-user are necessary.
			\par
		
		\subsection{Data cleansing}
			It was observed that numerous Tweets were being posted more than once and in several occurrences even hundreds of times. These duplicates were being primarily posted by bots, as was evident from a short observation of user profiles belonging the Tweets original posters. Evidently, additional effort was being made by the programmers of the bots to try and mask them by slightly altering the content of the Tweets, or the user account. This was usually done by changing or adding characters to the text, which carry no lingual significance in themselves. Moreover, in furtherance of increasing the bots' credibility as an actual people, often times entire nets of such bot could be observed, wherein the bots would maintain friendship and following connections among themselves. An example is shown in Figure \ref{fig:twitterbot}. This in turn, increases the amount of \textit{friends} and \textit{followers}, further contributing to their guise of real human users of Tweeter. Upon closer observation, such accounts reveal their true essence, since most of the content propagated by them is commercial in nature and is repeated verbatim time and again across many of the related accounts \textit{followers} and \textit{friends}. For this reason, it is safe to assume that no actual people are behind these accounts. 
			\par
			
			\begin{figure}[h]
				\centering
				\includegraphics[width=0.8\textwidth]{twitterbot.PNG}
				\captionsetup{width=0.8\textwidth}
				\caption[Fake Twitter Accounts]{Twitter accounts, which present themselves as actual people. \textit{Source: Twitter}}
				\label{fig:twitterbot}	
			\end{figure}
		
			\noindent
			Several precautions were undertaken to try and filter out such bots. A passive precaution which was implemented, was blacklisting users, which had priorly been observed posting Tweets, which were verbatim copies of other Tweets within the same query. The suspected users were added to a suspect database and content originating from them was ignored in future queries. Another action undertaken with the same purpose in mind was a retrospective cleanup of the collected Tweets, based on their content similarity. After closing a collection query, a maximum similarity measure for each Tweet in relation to all other recorded Tweets was calculated using a simple rule. Following, Tweets which were found to have a maximal similarity score to other previously captured Tweets  higher than a predetermined threshold were classified as non-unique copies and were disregarded. As a measure of similarity, the S\o rensen-Dice coefficient (\cite{sorensen1948method}) was implemented \footnote{The \textit{SequenceMatcher.ratio()} function from the difflib Python module}. The coefficient is demonstrated in Figure \ref{fig:sorenson_dice} where $|X|$ and $|Y|$ are the numbers of elements in given Tweets X and Y accordingly. A higher $ QS_{XY} $ value indicates larger conformity between the $ X $ and $ Y $ groups, and ranges from 0 (completely different) to 1 (identical). A round of cleanup using this procedure would usually reduce a data set by from one quarter and up to one half of its original size.
			
			\begin{figure}[h]
				\begin{center}
					$QS_{XY} = \dfrac{2|X\cap Y|}{|X|+|Y|},$ \hspace{10pt}  $QS_{XY} \in [0:1]$
				\end{center}
				\caption{S\o rensen-Dice coefficient}
				\label{fig:sorenson_dice}
			\end{figure}
		
	\subsection{Building Feature-Sets}
	\label{build_features}
		Once a list of labeled Tweets is obtained, the next stage is constructing a feature-set to be later passed on as input for training Machine Learning Classifiers. The features contained if the feature sets describe certain aspects and characteristics of a Tweet and its owner. Two main approaches to feature sets are either \textit{Word-Based} or \textit{Descriptive Features}. The \textit{Word-Based} approach has two notable sub-types, which will be implemented in this experiment, namely \textit{Bag-of-Words} and \textit{Bag-of-NGrams}.
	
	\subsubsection{Descriptive Features}
		\label{section:Descriptive_Features}
		Similar approaches were used to classify Information Credibility on Tweeter (\cite{castillo2011information}). In some sense, the mission of this study is also to quantify the quality of social media content, however, in a very subjective manner.
		\par	
		
		Every Tweet object originating from the Twitter database is a complex multi-layered structure, which  simultaneously describes the posted Tweet itself along with the user profile of its author and any additional media objects related to the Tweet. This approach to feature extraction focuses on deriving the implicit meaning or sense of the tweet, rather than analyzing the verbatim textual content. Two tiers of features are extracted. The former type describes the Tweet object, whereas the latter draws information about the author himself. These features are described in Table \ref{table:desc_features} and are explained in the following paragraphs. Appendix \ref{appn:desc_features_stats} describes the general statistics of these features. 
		
		\begin{table}[h]	
			\begin{center}
				\resizebox{1 \textwidth}{!}{
					\begin{tabular}{c |l| l} 
						\hline
						\hline
						\multicolumn{1}{c|}{\multirow{2}{*}{Scope}} & \multicolumn{1}{|c}{\multirow{2}{*}{Feature}} & \multicolumn{1}{|c}{\multirow{2}{*}{Description}}\\ 
						&&   \\ 
						\hline
						\multirow{17}{*}{Message}& NUM\_STOP\_WORDS & Number of Stop-Words in the Tweet  \\
						& DUPLICATE					& An almost identical tweet was captured in same query  \\
						& LEN\_CHARACTERS			& Number of characters in Tweet text  \\
						& NUM\_WORDS				& Number of words in Tweet Text \\
						& HAS\_QUESTION\_MARK		& Tweet contains question mark  \\
						& HAS\_EXCLAMATION\_MARK	& Tweet contains exclamation mark  \\
						& HAS\_MULTI\_QUEST\_EXCLAM & Tweet contains either multiple question marks or exclamation marks \\
						& EMOJI\_SENT				& Sentiment of emoji symbols. TRUE for positive \\
						& EMOJI\_SENT\_SCORE		& Sentiment score magnitude of emoji symbols. Always non-negative  \\
						& HAS\_PRONOUN				& Tweet contains pronouns \\
						& COUNT\_UPPER				& Percent of upper case letters out of Tweet text \\
						& HAS\_HASHTAG				& Tweet contains Hashtags\\
						& RETWEET\_COUNT			& The number of times the Tweet was reposted\\
						& URLS\_WIKI				& The Tweet contains a URL, belonging to Wikipedia's top 100\\
						& URLS\_MOZ					& The Tweet contains a URL, belonging to Moz's top 500\\
						& SENTIMENT					& Tweet's sentiment score\\
						& SENTIMENT\_CONF			& Tweet's sentiment score confidence level\\
						\hline
						\multirow{8}{*}{User}	& REG\_AGE 	& Number of days since user registered on Twitter\\
						& STATUS\_CNT 				& Number of Status changes			\\
						& FOLLOWERS\_CNT 			& Number of Followers the user has	\\
						& FRIENDS\_CNT				& Number of Friends the user has	\\
						& VERIFIED					& Whether User account is verified by Twitter\\
						& HAS\_DESC 				& Whether User account contains a description\\
						& HAS\_URL					& Whether User account has a homepage URL	\\
						& MSG\_P\_DAY 				& Average number of messages posted per day	\\
						\hline\hline
					\end{tabular}
				}	
			\end{center}
			\caption{Descriptive Features}
			\label{table:desc_features}
		\end{table}
	
		\par
		
		This method offers an alternative approach to features, differing from the other two, by being based on a more generalized view of the Tweet. Instead of concentrating on the actual textual content, other non direct properties are observed. Descriptive features are aimed at describing the Tweets implicit properties, such as attitude, sentiment, seriousness and trustworthiness. These features detect the presence of different symbols, their frequency and consecutiveness. Additionally, unlike the Word-Based approach, non-textual objects such as multimedia, links and mentions of other users and Tweets are also taken into account. Furthermore, features of a Tweets' owner are also included in the input. Since Tweeter's API provides a complete user profile incorporated inside the Tweet data object itself, constructing features describing the user is done simultaneously to features describing the content of the Tweet itself. This approach might be viewed as an \textit{indirect} one, since less obvious properties of the Tweet are used to characterize it.
		
		\par
		
		Descriptive features can be divided into two distinctive groups. The first tier is text-based features. As the name suggests, these features will mostly denote the presence or lack of specific characters such as emoticons and signs in the Tweets text. Whether a Tweet contains combinations or sequences of certain symbols as well as ratios defining the text also befit this category.
		
		\par
		
		The second tier of features describe any special \textit{Entities}\footnote{Tweeter's nomenclature} contained within a Tweet. \textit{Entities} refer to non-textual contents of a Tweet, such as media (in form of pictures, audio or video), URL's linking to external websites, mentions or Retweets (referring to other Tweets or to Tweeter user profiles) and finally Hashtags. A word or phrase preceded by the Hashtag symbol \textbf{\#} indicates an association of web content (such as a Tweet or other micro-blogging post) to a specific theme. These could be an event, news, gossip or any other tidbit \footnote{Loose definition by \textit{Merriam-Webster}, \url{https://www.merriam-webster.com} }. Hashtags are used primarily to simplify looking up Tweets or other social media content by technically associating them with the \textit{hashtagged} topic. The following paragraphs elaborate on the construction and logic of some of the less obvious descriptive features.
		
		\paragraph{Number of Stop-Words}
			The total number of Stop-Words in the tweets' text. Stop-Words are those, which carry grammatical meaning but usually little to no contextual weight. These words are also usually the most common words in the corpus. The full list of English Stop-Words is available in Appendix \ref{appn:stop_words}. 
	
		\paragraph{Duplicate}	
			Tweets are queried from the servers using a search term. This feature will receive the value \textit{TRUE}, if another Tweet was found, which has a similarity score exceeding a predetermined threshold. The similarity of each pair of Tweets was calculated using the S\o rensen-Dice coefficient as in Figure \ref{fig:sorenson_dice}.
		
		\paragraph{Emoji Sentiment}
			All Emoji symbols are extracted from the Tweet and are scored according to the chart in the following URL: \url{http://kt.ijs.si/data/Emoji_sentiment_ranking/}. The ranking is based on a study by \cite{Kralj2015emojis}. In this paper, Emojis were labeled to have a positive, negative or neutral connotation. The feature EMOJI\_SENT denotes the sign of sentiment (positive or negative), whereas the feature EMOJI\_SENT\_SCORE denotes the strength of the sentiment. When several Emojis are found, their appropriate values are averaged out. 
	
		\paragraph{Retweet Count}
			Twitter content can be reposted as part of a new Tweet. This operation is named \textit{retweeting} in the Tweeter nomenclature. When a Twitter user wishes to reference another Tweet in its entirety, the enclosed (Retweet) Tweet will be framed inside the new Tweet and called a Retweet. Additionally, the original post keeps track tracks of the number of times it was reposted by other users. This acts as a measure of the Tweet's diffusion.
		
		\paragraph{Popular Domains}
			Tweets often contain links to objects both inside and outside the Tweeter realm. External items are usually sourced using a Uniform Resource Locators (URL). These URLs, in turn, belong to domains. The features URLS\_WIKI and URLS\_MOZ denote whether an enclosed URL belongs to one of the top 100 or top 500 most visited domains accordingly. The list of domains are publicly available and are regularly updated. Top 100 most popular domains by Alexa are available on Wikipedia\footnote{\hspace{0.01cm} Wikipedia: \url{https://en.wikipedia.org/wiki/List_of_most_popular_websites}} and the top 500 most popular domains are provided by MOZ\footnote{\hspace{0.01cm} Moz list: \url{https://moz.com/top500}}.
		
		\paragraph{Tweet Sentiment}
			The NLTK corpora contains a set of 10,000 Tweets labeled per hand to have either a positive or negative sentimental connotation. A Machine-Learning algorithm was trained using this set, to classify Tweets their sentiment. The classification in done using a Vote classifier. The feature SENTIMENT denotes the Tweet's sentiment (TRUE for positive or FALSE for negative) and SENTIMENT\_CONF denotes the number of underling classifiers of the Vote classifier, which voted for the aforementioned label.
		
		\paragraph{Status Count}
			On the Tweeter platform every post by submitted is regarded as a new \textit{Status} or a \textit{Status Change}. The feature indicated the number of such Statuses since the inception of the Tweeter account. This number includes both Tweets and Retweets.
		
		\paragraph{Verified}	
			User accounts are usually not limited when choosing a moniker to go by publicly on Tweeter. This leads to a plethora of accounts impersonating figures or organizations of public interest. Tweeter tries to verify the authenticity of accounts, which it is determines to be of public interest\footnote{\hspace{0.01cm} Verified Accounts: \url{https://support.twitter.com/articles/119135}}. Figure \ref{fig:verified_accounts} demonstrates a verified and a unverified Tweeter account of U.S. president Donald J. Trump. \textit{Source: Twitter}
		
	
		\begin{figure}[h]
			\centering
			\captionsetup{width=0.8\textwidth}
			\includegraphics[width=0.8\textwidth]{verified_account.png}
			\caption[Tweeter Verified Accounts]{
				\footnotesize{
					Tweeter verified (left) and unverified (right) accounts of U.S. president Donald J. Trump. Notice the blue badge denoting a verified account next to the profile name.
				}
			} 
			\label{fig:verified_accounts}
		\end{figure}
	
		\paragraph{Followers Count}
			The number of Tweeter users actively following this user. A high number of followers suggests, that the person may be views as an Opinion Leader or a person whose carries substantial influence on social media.
		
		\paragraph{Friends Count}
			This features is the mirror image of the Followers Count. It specifies the number of User accounts the current User is following.
	
	\subsubsection{Bag-of-Words}
	\label{sec:bag_of_words}
		This approach simply converts the entire text corpus to a frequency charts of all the words contained within. Words are then selected to act as features in incoming data, which is to be classified. The features are hence a variable list (usually of several thousands in length), where each variable is a boolean representation, indicating the presence or absence of a certain word. Usually the words undergo preprocessing as is common in Natural Language Processing prior to being used as features. The corpora are segmented to lists of words, often omitting articles, proposition and punctuation. Such grammatical structures are critical in human speech and written form in order to convey one's meaning clearly and explicitly, however for the purposes of more ambiguous classification as in our case, such nuances are avoided for the sake of simplicity. Words are then \textit{stemmed} or \textit{lemmatized}, meaning their are reverted to their grammatical stem - dropping all prefixes and suffixes. This eases the enumeration of words, since it is preferable that the same words in different inclinations would be counted as the same. For example, the word pair \textit{eating} and \textit{ate} would be reverted to their stem \textit{eat}, as well as \textit{apple} and \textit{apples} would be considered as one and the same. Finally, words would be assigned their part of speech (noun, verb, adjective ..) and could be either ignored or incorporated into the feature set, according to the conceived importance of a given part of speech.
	
		Therefore, this technique draws meaning from the verbal content of a corpus. From another perspective, one might say that each word carries certain information towards distinguishing different text samples along some trait. We strive to draw correlations between frequencies of certain words and classes to which, we wish to label the samples. Table \ref{table:BOW_top_words} shows most frequent words to be encountered in the corpus. 
	
		\begin{table}[H]
			\centering
			\begin{tabular}{ccc}
				\hline\hline
				&	Word 	& Frequency \\
				\hline
				1	&	amazon 	& 1857	\\
				2	&	food	& 248	\\	
				3	&	whole 	& 212	\\
				4	&	new		& 199	\\
				5	&	book	& 183	\\
				6	&	amp		& 161	\\
				7	&	echo 	& 155	\\
				8	&	free 	& 130	\\
				9	&	buy 	& 124	\\
				10	&	prime 	& 113	\\
				\hline\hline				
			\end{tabular}
			\qquad 
			\begin{tabular}{ccc}
				\hline\hline
				&	Word 	& Frequency \\
				\hline
				7889	&	medieval 		& 1	\\
				7890	&	npr				& 1	\\	
				7891	&	katerichards09 	& 1	\\
				7892	&	champ			& 1	\\
				7893	&	prosecutor		& 1	\\
				7894	&	khalids			& 1	\\
				7895	&	explains 		& 1	\\
				7896	&	commenting 		& 1	\\
				7897	&	george 			& 1	\\
				7898	&	lowcost 		& 1	\\
				\hline\hline				
			\end{tabular}
			\caption{Top 10 Most and Least Frequent Words Captured}
			\label{table:BOW_top_words}%
		\end{table}%
	
	\subsubsection{N-Grams}
		A further development on the Bag-of-Words concept. Using this approach, sequences of $ n $ words are extracted instead of single words and used as features. Such an approach has the added benefit of preserving some of contextual meaning of the text as well as discovering reoccurring phrases. This would prove even more beneficial is scenarios, in which the classification would be between positive and negative intonation, such as opinions and reviews. A down side to this method, is that much more feature are created therefore increasing the computational complexity. 
	
		\begin{table}[h]	
			\begin{center}
				\begin{tabular}{c l} 
					\hline\hline
					N &  \multicolumn{1}{c}{Possible N-Grams }  \\ 
					\hline
					2 & the movie ; movie was ; was not ; not great  \\
					3 & the movie was ; movie was not ; was not great \\
					4 & the movie was not ; movie was not great \\
					5 & the movie was not great \\ 
					\hline\hline
				\end{tabular}
			\end{center}
			\caption[N-Gram Example]{Possible N-grams of size 2 to 5 }
			\label{table:ngrams}
		\end{table}
	
		For example, using standard Bag-of-Words approach on the sentence "the movie was not great" results in 5 features \{the, movie, was, not, great\}, or even 2 after removing stop-words \{movie, great\}. When applying \textit{N-Grams} of sizes 2 to 5 on the same sentence, results in 10 features as seen in Table \ref{table:ngrams}.
		
		\par 
		
		Table \ref{table:top_ngrams} exemplifies the most and least often occurring N-Grams of sizes 2 - 5 words each. The 4-word phrase "is now on sale" appeared 50 times in the large Tweet corpus.
		
		\begin{table}[H]
			\centering
			\footnotesize
			\resizebox{1 \textwidth}{!}{
				\begin{tabular}{c| lc | lc }
					\hline\hline
					&  \multicolumn{1}{c}{2-Grams}	& Frequency & \multicolumn{1}{c}{3-Grams} & Frequency\\
					\hline
					1	&	on amazon 	& 938 		& have just listed 		& 260 		\\
					2	&	via amazon 	& 842 		& available on amazon	& 96 		\\	
					3	&	whole foods & 366  		& 399 via amazon		& 81 		\\ 
					4	&	of the		& 333 		& for 399 via			& 81 		\\
					5	&	in the		& 300  		& panini for 399		& 70 		\\
					6	&	just listed	& 282 		& buying whole foods 	& 65 		\\
					7	&	have just 	& 265 		& on sale for			& 60 		\\
					8	&	check out 	& 223  		& whole foods for 		& 59 		\\
					9	&	amazon prime& 218		& tune in buy			& 55 		\\
					10	&	amazon echo & 163  		& buypayoff amazon amazonproducts	& 55 		\\
					\hline\hline
					& \multicolumn{1}{c}{4-Grams} & Frequency & \multicolumn{1}{c}{5-Grams}	& Frequency \\
					\hline
					1	&for 399 via amazon  	& 81 & 2004 panini for 399 via 			   & 50	\\
					2	& panini for 399 via 	& 70 & is now on sale for 				   & 49	\\	
					3	& tune in buy it	 	& 55 &	germany euro 2004 panini for 	   & 47	\\ 
					4	& buypayoff amazon products bestbuy	& 55 &	might like pumpkinfarmer iartg novel & 46\\
					5	& 2004 panini for 399	& 50 & amazon is buying whole foods 	   & 43	\\
					6	& euro 2004 panini for 	& 50 & amazon to buy whole foods 		   & 31	\\
					7	& is now on sale	 	& 50 & pbscale ai big data cloud 		   & 23	\\
					8	& now on sale for		& 49 & check out this amazon deal		   & 22	\\
					9	& ai big data cloud		& 46 & ai big data cloud boot 			   & 21	\\
					10	& now available on amazon& 39& looking for professional book cover & 17	\\	
					\hline	
				\end{tabular}
			}
			\caption{Top 10 Most and Least Frequent N-Grams Captured}
			\label{table:top_ngrams}%
		\end{table}%
	
	\subsection{Labeling the Training Data}
		Labeling refers to the process of inspecting training samples manually by a real person and assigning a label, or affiliation of the data sample to one of the possible classes. The labeling of training examples was conducted over the length of several weeks by the author of this study. The labeling process was carried out unavoidably in a subjective manner. Apart from holding up to a few rules of thumb, it is hard to formally model the labeling pattern. This in fact, makes this an ideal task for Machine Learning, since the logic behind it is intuitive rather than strict.
			
	\subsection{Training Classifiers}
		Several configuration are to be applied when training the different classifiers, in the interest of quantifying and measuring the quality of classification. Hence, the size of the datasets used for training will vary, to observe the relation between set size and classifier accuracy. Additionally, different amounts of features should be tested, to determine whether accuracy grows linearly as the number of features increase or rather converges after some threshold. Furthermore, the training duration is to be measured to determine effectiveness of training as a time function.
		
		\par
		
		The different classifiers are grouped according to their designation and type. Additionally, another classifier type is presented. This supplementary new classifier incorporates all previous classifiers and classifies through a majority vote. This classifier will be referred to as Vote Classifier. An apparent drawback of the Vote Classifier, is the classification duration which will be as long as the sum of classification durations of all its components.
		
		