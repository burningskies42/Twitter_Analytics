\section{Conclusion}	
	The sphere of influence of the Internet as a news source has expanded rapidly in the last few years. In particular, social networks have been out-competing traditional outlets, due to their abundance of constantly updated information, which is laid at the fingertips of practically any computer user. However, unlike conventional old-fashioned resources the likes of television and printed press, publishers of Internet content are not subjected to rigorous review. As a result, such publishers are not bound by reliability and accountability for their content. Any private individual is free to spread any sort content she wishes, as long as no state laws are broken. Another perspective of this widely consumed content is not its factual truthfulness but rather its relevance and subjective interest for the user. Veritably, the user experience of social networks could be greatly improved by customizing the posts which is presented to them. Furthermore, this customization is to be done in real-time and on a per-user basis.
	
	\par
	The data used for this experiment was collected automatically from an API provided publicly by Tweeter. The collection in itself did not pose any difficulty. However, the quality of the content obtained was of low quality. The data was riddled with numerous reposted copies and advertisement. In fact, the very type of content that I wished to reduce in a users data-feed. For this reason, the data was cleaned by removing copies and obviously commercial posts. The process of cleaning was also automated, so that future collection of data we already be able to avoid low-quality content, as it is being gathered. The resulting data had to be than manually labeled by an actual person to be used as input for the machine learning algorithms. This process proved to very time consuming leading to a moderately sized dataset.
	
	\par
	
	This research analyzed different machine learning algorithms as well as their construction for the purpose of adjusting a subjective user-interest scheme. I was established that the different algorithms perform very comparably. A compound voted classifier, composed of sub-classifiers - each trained using another method, was also experimented on. Such a classifier provided more stable and constant results but a much higher computational price. Moreover, the theoretically simpler models proved to perform slightly better, which is promising since they are usually computationally also simpler, namely Logistic Regression and Naive Bayes. 
	
	\par
	
	Several techniques of feature extraction were attempted. Descriptive features did not manage to show encouraging results and did not appear to be reach significant forecasting irregardless of configurations. The N-Grams approach performed far better and showed promising results, reacting positively to larger dataset. Future experiments should try and harvest more data, since the potential of the algorithms did not seem to peak with the available dataset.
	
	\subsection{Genaral Use Cases}
		{\color{red} \Large placeholder}
		
	\subsection{SWOT}
	{\color{red} \Large placeholder}