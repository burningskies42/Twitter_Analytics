\section{Conclusion}	
	The utilization of the Internet as a news source has expanded rapidly in the last few years. In particular, social networks have been out-competing traditional outlets, due to their abundance of constantly updated information, which is laid at the fingertips of practically any computer user. However, unlike conventional old-fashioned resources the likes of television and printed press, publishers of Internet content are not subjected to rigorous review. As a result, such publishers are not bound by reliability and accountability for their content. Any private individual is free to spread any sort of content she wishes, as long as no laws are infringed upon. Another perspective of this widely consumed content is not its factual truthfulness but rather its relevance and subjective interest for the user. Analogously, traditional media offers journals and TV programs covering a specific topic. Veritably, the user experience of social networks could be greatly improved by customizing the posts which is presented to them to fit their personal interest scheme. Furthermore, this customization is to be done in real-time and on a per-user basis to maximally improve the user experience.
	
	\par
	The data used for this experiment was collected automatically from an API provided publicly by Tweeter. The collection in itself did not pose any difficulty. However, the quality of the content obtained was inherently low. By way of illustration, the data was riddled with numerous reposted copies of the same Tweets as well as a plethora of advertisement material. In fact, the very type of content that I wished to reduce in a users data-feed was significantly present. For this reason, the data was cleaned by removing copies and obviously commercial posts. The process of cleaning was also automated, so that during future collection of data, low-quality content would be avoided, as it is being gathered in real-time. Furthermore, posts containing words obviously associated with commercials (Giveaway, Giftcard etc.) were also removed. The resulting data had to be than manually labeled by a person to be used as input for the machine learning algorithms. This process proved to very time consuming resulting in a moderately sized dataset. This holds especially true for weighted datasets. For the accomplishment of a weighted dataset, the ratio of occurrence between both types of classes had to be roughly 1:1. In reality, a sample of class \textit{news} appeared about once for every seven or eight \textit{not-news} items. Thus, when experimenting with a weighted dataset, out of the around 13000 samples available only less than 3000 could be used, in order to maintain the 1:1 ratio of \textit{news}:\textit{not-news}. Nonetheless, classifiers trained on a weighted dataset proved to vastly outperform the ones trained on the much larger not weighted set. Moreover, a key benefit of such an approach is that once the manually labeled dataset allows for sufficiently accurate classifiers, the data can be collected, labeled and used as training material by the machine for future iterations of the classifier. Thus, the machine becomes self improving.
	
	\par
	
	This research analyzed different machine learning algorithms as well as their construction for the purpose of adjusting a subjective user-interest scheme. Various aspects of training were inspected. Primarily, the cost of construction (in terms of duration) and accuracy. It was established that the different algorithms perform very comparably. A compound voted classifier, composed of sub-classifiers - each trained using another method, was also experimented on. Such a classifier yielded more stable and consistent results but at a substantially higher computational cost. Moreover, the theoretically simpler models proved to produce slightly higher accuracy scores, which is promising since their computational simplicity provides intuition to the process, namely Logistic Regression and Naive Bayes.
	
	\par
	
	Several techniques of feature extraction were examined. Descriptive features did not manage to show encouraging results and did not appear to reach significant forecasting irregardless of configurations. This might be due to fuzzy or ambiguous labels. Additionally, Descriptive features were constructed in 2 tiers, \textit{Tweet} and \textit{User}. In other literature (\cite{castillo2011information}) an additional tier \textit{Topic} was used, through which numerous samples were grouped together and features describing the contents of the group were extracted. Possibly, this additional tier explains the variation missing in this study.
	The N-Grams approach performed exceedingly better and showed promising results, reacting positively to larger datasets. Future experiments should try and harvest more data, since the potential of the algorithms did not seem to peak with the available dataset.
	
	\subsection{Theory and Practice}
		The methodology presented here was shown to be a functional tool for the task of classifying social network content to better suit the preferences of individual users. Through the same process and with practically unchanged training procedures, the algorithms might be adjusted to gather useful content for different interest groups with various goals.
		
		\par 
		
		One such possible example is observing the public perception of a company or a newly released product. The technique developed here would allow to closely monitor relevant posts and through an extra step summarize the public Sentiment (\cite{go2009twitter}) regarding the company or product. This could be used by investors, who consider investing in a company of branch, or by the company itself as an instrument for feedback from the public. In cases of low public interest, more marketing resources could be invested.
	
	\subsection{Strengths and Weaknesses}
		The algorithm presents significant classification power but an initial phase of labeling is rather time consuming,  meaning a base line classifier must be first completed as a \textit{vanilla} classifier. This \textit{vanilla} classifier could be later integrated into the social network user account, where it could mine further knowledge regarding the preferences of unique users behind the scenes. Thus, the algorithm can not be deployed fully customized but rather generally tuned for specific population segments sharing similar characteristics and be further adjusted as more data is collected about the habits of individual users. 
		
		\par 
		
		The methods described in this study describe classification of extremely \textit{fuzzy} or vaguely defined classes. As such, the machine is tasked with mimicking human intuition. This scenario requires large masses of heterogeneous data to succeed. The procurement and transformation of such data is time extensive. Subsequently, avoidance of the classical over-fitting problem of machine learning classification proves to be a challenge. In other works, finding the balance between filtering out too much or too little content is another obstacle. On the other hand, since the algorithm is theoretically self-improving the problem of over-fitting or rather adjusting for the user's preference should be resolved or at least alleviated over time. 
		
	\subsection{Future Research}
		The general classification theme was tried on a single platform, Tweeter that is. As such, the study was subjected to the unique system, which characterizes it, including the post size limit and the ability to embed objects and different media in post. A similar platform often used interchangeably with Twitter is Reddit, which also sees wide use as a social news source. Using Reddit might prove beneficial because of its more simplistic and less closely monitored interactions between the users. Another possible source of social network content is stockTwits, which is a platform closely resembling the design and interface of Twitter, but concentrating thematically on the field of business and financial markets. StockTwits might therefore prove to be an essential resource in constructing tools for investors following trends around traded companies.
		
		\par 
		
		An additional direction, which was out of scope for this study, is unsupervised learning. This approach could prove to be simple in the preprocessing stage. The data must not be labeled and categorized before passed on as input for the algorithms. Moreover, a clustering algorithm might shed light on possible groupings of data previously not explored. Furthermore, these grouping are not bound by the binary class system of the current study, further segmenting the data into finer subcategories.
		
		\par 
		
		As mentioned priorly, the descriptive feature approach did not present satisfactory classification power. In part, possibly due to not enough features being extracted. A possible solution would be to segment the collected data into subgroups by topic, time or source. This segmentation could even be conducted by clustering them automatically. Then an additional tier of feature regarding the topic could be extracted. Namely, another layer of information about details inside the topic could be harvested.
		
		\par 
		
		For the purpose of both the \textit{Bag-of-Words} and \textit{N-Grams} approaches word frequencies were calculated by simply counting them across the entire data corpus. Another technique called \textit{TF-IDF} could be used instead of regular frequency, if the data was in fact grouped into smaller clusters called \textit{documents} as mentioned before. \textit{TF-IDF} is a measure consisting of two metrics. Namely, Term-Frequency $ TF(t,d) $ denotes the frequency of term $ t $ in document $ d $ and Inverse-Document-Frequency $ IDF(t,D) $ denotes the inverse frequency of documents containing the term $ t $ out of all documents in the corpus $ D $. This measure takes into account the fact, that some words appear often inside few documents but seldom inside the corpus or vice versa. Thus, their appropriate weights are adjusted accordingly.
		
		\par 
		
		The calculation behind Machine Learning algorithms can be oftentimes lengthy and complicated. An option which could greatly improve performance is Multi-Processing. Most modern computers contain CPUs equipped with several cores, however by default, the computes often makes use of a single core for calculating. Training classifiers involves multitudes of calculations, independent of one another. These calculations could be streamlined and conducted in parallel on several of the CPU cores. Multi-processing could strain a computer responsible for numerous processes, slowing down its overall performance per application However, when a dedicated machine would be used for this purpose alone, no such hindrance would occur. This technique was even shortly tried in this study, but was not fully researched. However, the preliminary results seemed promising and allowed training several classifiers in parallel instead of consecutively.