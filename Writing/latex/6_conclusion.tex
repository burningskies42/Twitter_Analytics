\section{Conclusion}	
	The utilization of the Internet as a news source has expanded rapidly in the last few years. In particular, social networks have been out-competing traditional outlets, due to their abundance of constantly updated information, which is laid at the fingertips of practically any computer user. However, unlike conventional old-fashioned resources the likes of television and printed press, publishers of Internet content are not subjected to rigorous review. As a result, such publishers are not bound by reliability and accountability for their content. Any private individual is free to spread any sort of content she wishes, as long as no laws are infringed upon. Another perspective of this widely consumed content is not its factual truthfulness but rather its relevance and subjective interest for the user. Analogously, traditional media offers journals and TV programs covering a specific topic. Veritably, the user experience of social networks could be greatly improved by customizing the posts which is presented to them to fit their personal interest scheme. Furthermore, this customization is to be done in real-time and on a per-user basis to maximally improve the user experience.
	
	\par
	The data used for this experiment was collected automatically from an API provided publicly by Tweeter. The collection in itself did not pose any difficulty. However, the quality of the content obtained was inherently low. By way of illustration, the data was riddled with numerous reposted copies of the same Tweets as well as a plethora of advertisement material. In fact, the very type of content that I wished to reduce in a users data-feed was significantly present. For this reason, the data was cleaned by removing copies and obviously commercial posts. The process of cleaning was also automated, so that future collection of data we already be able to avoid low-quality content, as it is being gathered. Furthermore, post containing words obviously associated with commercials (Giveaway, Giftcard etc.) were also removed. The resulting data had to be than manually labeled by a person to be used as input for the machine learning algorithms. This process proved to very time consuming leading to a moderately sized dataset. However, a key benefit of such an approach is that once the manually labeled dataset allows for sufficiently accurate classifiers, the data can be collected, labeled by the machine and used as training material for future iterations of the classifier. Thus, the machine becomes self improving.
	
	\par
	
	This research analyzed different machine learning algorithms as well as their construction for the purpose of adjusting a subjective user-interest scheme. Various aspects of training were inspected. Primarily, the cost of construction (in terms of duration) and accuracy. It was established that the different algorithms perform very comparably. A compound voted classifier, composed of sub-classifiers - each trained using another method, was also experimented on. Such a classifier provided more stable and yielded consistent results but at a substantially higher computational cost. Moreover, the theoretically simpler models proved to produce slightly higher accuracy scores, which is promising since their computational simplicity provides intuition to the process, namely Logistic Regression and Naive Bayes. 
	
	\par
	
	Several techniques of feature extraction were examined. Descriptive features did not manage to show encouraging results and did not appear to be reach significant forecasting irregardless of configurations. This might be to the very fuzzy or ambiguous labels. The N-Grams approach performed exceedingly better and showed promising results, reacting positively to larger datasets. Future experiments should try and harvest more data, since the potential of the algorithms did not seem to peak with the available dataset.
	
	\subsection{Theory and Practice}
		The methodology presented here was shown to be a functional tool in th task of classifying social network content to better suit the preferences of individual users. Through the same process and with practically unchanged training procedures, the algorithms might be adjusted to gather useful content for different interest groups with various goals.
		
		\par 
		
		One such possible example is observing the public perception of a company or a newly released product. The technique developed here would allow to closely monitor relevant posts and through an through an extra step summarize the public Sentiment (\cite{go2009twitter}) regarding the company or product. This could be used by investors, who consider investing in a company of branch, or by the company itself as an instrument for feedback from the public.
	
	\subsection{Strengths and Weaknesses}
		The algorithm presents significant classification power but an initial phase of labeling is rather time consuming meaning a base line classifier must be first completed as a vanilla classifier. This vanilla classifier could be later integrated into the social network user account, where it could behind the scenes to mine further knowledge regarding the preferences of unique users. Thus, the algorithm can not be deployed fully customized but rather generally tuned for specific population segments sharing similar characteristics and further adjusted as more data is collected about the habits of the user. 
		
		\par 
		
		The methods described in this study describe classification of extremely \textit{fuzzy} or vaguely defined classes. As such, the machine is tasked with mimicking human intuition. This scenario requires large masses if heterogeneous data to succeed. The procurement and transformation of such data time extensive. Subsequently, avoidance of the classical over-fitting problem of machine learning classification proves to be a challenge. In other works, finding the balance between filtering out too much or too little content is another obstacle. On the other hand, since the algorithm is theoretically self-improving the problem of over-fitting or rather adjusting for the user's preference should be resolved or at least alleviated over time. 
		
	\subsection{Future Research}
		The general classification theme was tried on a single platform, Tweeter that is. As such, the study was subjected to the unique system, which characterizes it, including the post size limit and the ability to embed objects and different media in post. An alternative system which is often used in a similar fashion is Reddit, which also is widely used social news source. Using Reddit might prove beneficial because of its more simplistic and less closely monitored interactions between the user. Another possible source of social network content is stockTwits, which is a platform closely resembling the design and interface of Twitter, but concentrating thematically on the field of business and financial markets. StockTwits might therefore prove to be an essential resource in constructing tools for investors following trends around traded companies.
		
		\par 
		
		An additional direction, which was out of scope for this study, is unsupervised learning. This approach should be prove to be simple in the preprocessing stage. The data must not be labeled and categorized before passed on as in put for the algorithms. Moreover, a clustering algorithm might shed light on possible groupings of data previously not explored. Furthermore, these grouping are not bound by the binary class system of the current study, further segmenting the data into finer subcategories.