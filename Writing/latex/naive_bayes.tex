\subsubsection{Naive Bayes}
	Probably the most common classification algorithm and usually the go-to classifier when handling text classification tasks (\cite{rish2001empirical}). The Naive Bayes algorithm is popular because of its propensity to perform as well as other much complexer classifiers, if not outperform them. This holds true, despite the overly simplified or "naive" assumptions being made at its core. This algorithm is prevailing for numerous classification problems ranging from sentiment analysis, spam filtration and up to classifying data to user-defined classes as is the case of this study. The attitude adopted in this scenario is probabilistic, since the algorithm assigns labels to observations according to which class  they are most likely to belong to, given their features. The designation of the algorithm as "Naive" stems from the base assumption, which is that the occurrence of a certain feature in a sample is independent from the occurrences of other features in the same sample. As such, the probabilistic calculus is as in \ref{nb_calc}, where $ \textbf{X} = (x_1,...,x_n) $ is a vector of the data features and $ C $ is the appropriate class. The probability of occurrence of vector $ X $ given a certain class $ C $ is solely dependent on the geometric sum of probabilities for the $ x_i$   $\forall i \in [1:n] $.
	
	
	\begin{equation}
		P(\textbf{X}|C) = \prod_{i=1}^n P(x_i|C)
		\label{nb_calc}
	\end{equation}
	
	\paragraph{Classification}
		At its heart, the algorithm is based on Bayes theorem (Equation \ref{nb_bayes}), which allows for the calculation of conditional probability. In the case of classification, it is the likelihood that a novel data point belong to a given class $ C_i $, given its describing properties (features) $ \textbf{X} $.
	
	\begin{equation}
		P(A|B) = \frac{P(X|A) \cdot P(A)}{P(B)}
		\label{nb_bayes}
	\end{equation}
	
		The algorithm thus calculates the posterior probability of the data point to be part of each given class $ i $  out of $ k $ classes. After which these probabilities are than compared [\ref{nb_post_prob}]. The classification of the new point is determined by which class is most likely (has the highest probability). In pursuance of calculating these likelihoods, we must first calculate the numerator of [\ref{nb_post_prob}] as in [\ref{nb_prob_numerator}].
	
	\begin{equation}
		\begin{aligned}
			P(C_i|x_1, ... , x_n) &= \frac{P(x_1, ... , x_n |C) \cdot P(C_i)}{P(x_1, ... ,x_n)} \\
			& \ \forall \ \ \  1 \leq \ i \leq \ k
		\end{aligned}
		\label{nb_post_prob}
	\end{equation}
	
		In [\ref{nb_prob_numerator}] we derive that the conditional probability term, $ P(x_j|x_{j+1}, ...,x_n,C_i) $ is equal to $ P(x_j|C_i) $ based on the \textit{naive} assumption that the occurrence of certain features $ x_j $ is independent of the occurrences of all other features $ (x_1,x_2,..., x_{j-1},x_{j+1}, ...,x_n) $ (see Equation \ref{nb_calc}).
	
	\begin{equation}
		\begingroup\makeatletter\def\f@size{8}\check@mathfonts
			\begin{aligned}
				P(x_1,x_2, ..., x_n |C) \cdot P(C_i) &= P(x_1,x_2,...,x_n,C_i) \\
				P(x_1,x_2, ..., x_n,C_i) &= P(x_1|x_2, ...,x_n,C_i) \cdot P(x_2, ...,x_n,C_i) \\
				&= P(x_1|x_2, ...,x_n,X_i) \cdot P(x_2|x_3, ..., x_n, C_i) \cdot P(x_3, ..., x_n,C_i) \\
				&= \ ... \\
				&= P(x_1|x_2, ...,C_i) \cdot P(x_2|x_3, ...,C_i) \cdot ... \\
				& \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 
				... \cdot P(x_{n-1}|x_n, ...,C_i) \cdot P(x_n|C_i) \cdot P(C_i)
			\end{aligned}
		\endgroup
		\label{nb_prob_numerator}
	\end{equation}
	
		The calculation in equation[\ref{nb_prob_numerator}] allows us to formulate the classification results in equation [\ref{nb_final}].
	
	\begin{equation}
		\begin{aligned}
			P(C_i|x_1,x_2, ...,x_n) &= \Bigg(
			\prod_{j=1}^n  P(X_j|C_i)
			\Bigg) \cdot
			\frac{P(C_i)}{P(x_1,x_2, ..., x_n)}\\ 
			&\ \forall \ \  1 \leq i \leq k \\
			\text{The expression } P(&x_1,x_2, ...,x_n)\text{ is constant for all classes } j: \\
			P(C_i|x_1,x_2, ...,x_n)& \propto \Bigg(
			\prod_{j=1}^n  P(X_j|C_i)
			\Bigg) \cdot P(C_i) \\
			&\ \forall \ \  1 \leq i \leq k 
		\end{aligned}
		\label{nb_final}
	\end{equation}
	
	\paragraph{Variations of the Algorithm}
		Several derivative forms of the basic Naive Bayes algorithm are also common. These variations usually implement different distributions on the feature probability $ P(x_j|C_i) $, thus adding a degree of sophistication to this simplistic idea.
		
		\subparagraph{Gaussian}
			A natural assumption when handling continuous data is that the values associated with each class are also continuous and distributed normally. De facto, the training data is first segmented according to the sample labels $ c_i$   $\forall  i \in [1:m] $, with $ m $ classes. Following, the mean and variance of each continuous variable (\textit{feature}) $ x_j $ $\forall  j \in [1:n] $ are calculated for each class  $ c_i $, with $ n $ \textit{features}. Let $ \mu_i = (\mu_{i1},\mu_{i2}, ... ,\mu_{in}) $ and $ \sigma_i^2 = (\sigma_{i1}^2,\sigma_{i1}^2 ,..., \sigma_{in}^2) $ be the vectors of means and variances of the feature set $ x_i $ associated with class $ C_i $ accordingly. The algorithm assumes a normal (Gaussian) distribution for the features, where as the parameters $ \sigma_{ij} $ and $ \mu_{ij} $ are derived from a maximum likelihood estimation for class $ C_i $ and \textit{feature} $ x_j $, i.e. Equation \ref{nb_gauss}.
	
		\begin{equation}
			P(x_{ij}|C_i) = \frac{1}{\sqrt{\vphantom{\frac{1}{1}} 2 \pi \sigma^2_{ij} c_i}} \cdot 
			\scaleobj{2}{e}^{ \displaystyle - \frac{(x_{ij}-\mu_{ij} c_i)^2}{2 \sigma^2_{ij} c_i} }
			\label{nb_gauss}
		\end{equation}
	
		\subparagraph{Multinomial}
			In a multinomial setting, the features represent the frequencies of word occurrences which are distributed according to a multinomial distribution. Hence, $ p_i $ is the probability that an event $ i $ occurs in the multinomial $ (p_1,p_2, ...,p_n) $. The feature set $ \textbf{x} = (x_1,x_2, ...,x_n) $ can than be visualized as a histogram, in which the frequency of event $ i $ (appearance of word $ i $ in a given data point) is represented by the height of the appropriate column. This method is especially common when undertaking the \textit{Bag-of-Words} approach. This will be further demonstrated in the Application part. Therefore, the likelihood of generating a histogram \textbf{x} is represented in Equation \ref{nb_multinom}.
	
	\begin{equation}
		P(X|C_k) = \frac{(\sum_i)!}{\prod_i x_i !} \cdot \prod_i p_{ki}^{x_i}
		\label{nb_multinom}
	\end{equation}
	
		Extrapolating Equation \ref{nb_multinom} into log-space, we get Equation \ref{nb_multinom_log}, where $ b = log \big( P(C_k)\big) $ and $ \textbf{w}_{ki} = log \big( p_{ki} \big) $. It is now evident that the classifier becomes linear in logarithmic-space.
	
	\begin{equation}
		\begin{aligned}
			log \big[ P(X|C_k) \big] &\propto log \bigg( P(C_k)\prod_{i=1}^n p_{ki}^{x_i} \bigg) \\
			&= log \bigg(P(C_k) \bigg) + \sum_{i=1}^n x_i \cdot log(p_{ki}) \\
			&= b + \textbf{w}_k \cdot \textbf{x} 
		\end{aligned}
		\label{nb_multinom_log}
	\end{equation}
		
		Additionally, the product must be adjusted for the case when a certain event (appearance of word $ i $) does not occur. Since non-occurrence will be denoted as a frequency equal to zero, such values must be smoothed out to prevent them from nullifying the entire equation.
	
	\subparagraph{Bernoulli}
		This variant assumes multivariate Bernoulli distributions for the features. Particularly, each feature can be represented with a boolean variable. Hence, the classifier requires the feature sets to be passed in the form of vectors composed of binary variables each representing a feature. The classification is based on the decision rule in Equation \ref{nb_bernoulli}, for class $ C_k $ and feature $ x_i $. The probability of class $ C_k $ producing the feature $ x_i $ is denoted by $ p_{ki} $. We can see that the non-occurrence problem from the Multinomial variant is being explicitly addressed here by penalizing for non-occurrence of a given feature $ x_i $.
	
	\begin{equation}
		P \big(\textbf{x}|C_k \big) = \prod p_{ki}^{x_i}(1-p_{ki})^{(1-x_i)}
		\label{nb_bernoulli}
	\end{equation}
	
	\newpage