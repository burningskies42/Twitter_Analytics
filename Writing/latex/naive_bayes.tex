\subsubsection{Naive Bayes}
	Probably the most common classification algorithm and usually the go-to classifier when handling text classification tasks [\cite{rish2001empirical}]. The Naive Bayes algorithm is popular because its propensity to perform as good as much complexer classifiers, if not outperform them. This holds true, despite the overly simplified or "naive" assumptions being made at its core. This algorithm is prevailing for numerous classification problems ranging from sentiment analysis, spam filtration and up to classifying data to user-defined classes as is the case in this study. The attitude adopted in this scenario is probabilistic, since it classifies observations according to which class are they most likely to belong to, given their features. 
	The word "Naive" in its name come from its base assumption, which is that the occurrence of a certain feature in a data point is independent from the occurrences of other features. As such, the probabilistic calculus is as in [\ref{nb_calc}], where $ \textbf{X} = (x_1,...,x_n) $ is a vector of the data features and $ C $ is the appropriate class.
	
	\begin{equation}
		P(\textbf{X}|C) = \prod_{i=1}^n P(x_i|C)
		\label{nb_calc}
	\end{equation}
	
	\paragraph{Classification}
		At its heart, the algorithm is based on Bayes theorem [\ref{nb_bayes}], which allows for the calculation of conditional probability. In the case of classification, it is the likelihood that a novel data point belong to a given class $ C_i $, given its describing properties (features) $ \textbf{X} $.
	
	\begin{equation}
		P(A|B) = \frac{P(X|A) \cdot P(A)}{P(B)}
		\label{nb_bayes}
	\end{equation}
	
		The algorithm thus calculates the posterior probability of the data point to be part of each given class $ i $  out of $ k $ classes. After which these probabilities are than compared [\ref{nb_post_prob}]. The classification of the new point is determined by which class is most likely (has the highest probability). In pursuance of calculating these likelihoods, we must first calculate the numerator of [\ref{nb_post_prob}] as in [\ref{nb_prob_numerator}].
	
	\begin{equation}
		\begin{aligned}
			P(C_i|x_1, ... , x_n) &= \frac{P(x_1, ... , x_n |C) \cdot P(C_i)}{P(x_1, ... ,x_n)} \\
			& \ \forall \ \ \  1 \leq \ i \leq \ k
		\end{aligned}
		\label{nb_post_prob}
	\end{equation}
	
		In [\ref{nb_prob_numerator}] we derive that the conditional probability term, $ P(x_j|x_{j+1}, ...,x_n,C_i) $ is equal to $ P(x_j|C_i) $ based on the assumption that the occurrence of certain features $ j $ is independent of the occurrences of all other features $ (x_1,x_2,..., x_{j-1},x_{j+1}, ...,x_n) $ [\ref{nb_calc}].
	
	\begin{equation}
		\begingroup\makeatletter\def\f@size{8}\check@mathfonts
			\begin{aligned}
				P(x_1,x_2, ..., x_n |C) \cdot P(C_i) &= P(x_1,x_2,...,x_n,C_i) \\
				P(x_1,x_2, ..., x_n,C_i) &= P(x_1|x_2, ...,x_n,C_i) \cdot P(x_2, ...,x_n,C_i) \\
				&= P(x_1|x_2, ...,x_n,X_i) \cdot P(x_2|x_3, ..., x_n, C_i) \cdot P(x_3, ..., x_n,C_i) \\
				&= \ ... \\
				&= P(x_1|x_2, ...,C_i) \cdot P(x_2|x_3, ...,C_i) \cdot ... \\
				& \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 
				... \cdot P(x_{n-1}|x_n, ...,C_i) \cdot P(x_n|C_i) \cdot P(C_i)
			\end{aligned}
		\endgroup
		\label{nb_prob_numerator}
	\end{equation}
	
		The calculation in equation[\ref{nb_prob_numerator}] allows us to formulate the classification results in equation [\ref{nb_final}].
	
	\begin{equation}
		\begin{aligned}
			P(C_i|x_1,x_2, ...,x_n) &= \Bigg(
			\prod_{j=1}^n  P(X_j|C_i)
			\Bigg) \cdot
			\frac{P(C_i)}{P(x_1,x_2, ..., x_n)}\\ 
			&\ \forall \ \  1 \leq i \leq k \\
			\text{The expression } P(&x_1,x_2, ...,x_n)\text{ is constant for all classes } j: \\
			P(C_i|x_1,x_2, ...,x_n)& \propto \Bigg(
			\prod_{j=1}^n  P(X_j|C_i)
			\Bigg) \cdot P(C_i) \\
			&\ \forall \ \  1 \leq i \leq k 
		\end{aligned}
		\label{nb_final}
	\end{equation}
	
	\paragraph{Variations of the Algorithm}
		Several derivative forms of the basic Naive Bayes algorithm are also common. These variations usually implement different distributions on the feature probability $ P(x_j|C_i) $, thus adding a degree of sophistication to this simplistic idea.
		
		\subparagraph{Gaussian}
			A natural assumption when handling continuous data is that the values associated with each class are also continuous and distributed normally. De facto, the data is first segmented by the class $ c_i $. Following, the mean and variance of each continuous variable $ x_j $ are calculated for each class  $ c_i $. Let $ \mu_i $ and $ \sigma_i^2 $ be the mean and variance of feature set $ x $ associated with class $ C_i $ accordingly. The algorithm assumes a normal (Gaussian) distribution for the features, where as the parameters $ \sigma_i $ and $ \mu_i $ are derived from a maximum likelihood estimation, i.e.,
	
		\begin{equation}
			P(x_j|C_i) = \frac{1}{\sqrt{\vphantom{\frac{1}{1}} 2 \pi \sigma^2 c_i}} \cdot 
			\scaleobj{2}{e}^{ \displaystyle - \frac{(x_j-\mu c_i)^2}{2 \sigma^2 c_i} }
		\end{equation}
	
		\subparagraph{Multinomial}
			Here the features represent the frequencies of word occurrences which are distributed in a multinomial fashion such that $ p_i $ is the probability that an event $ i $ occurs in the multinomial $ (p_1,p_2, ...,p_n) $. The feature set $ \textbf{x} = (x_1,x_2m ...,x_n) $ can than be visualized as a histogram, in which the frequency of event $ i $ (appearance of word $ i $ in a given data point) is represented by the hight of the appropriate column. This method is especially common when undertaking the Bag of Words approach. This will be further demonstrated in the Application part. Therefore, the likelihood of generating a histogram \textbf{x} is represented in [\ref{nb_multinom}].
	
	\begin{equation}
		P(X|C_k) = \frac{(\sum_i)!}{\prod_i x_i !} \cdot \prod_i p_{ki}^{x_i}
		\label{nb_multinom}
	\end{equation}
	
		When extrapolated into log-space, [\ref{nb_multinom}] turns into [\ref{nb_multinom_log}], where $ b = log \big( P(C_k)\big) $ and $ \textbf{w}_{ki} = log \big( p_{ki} \big) $. It is now evident that the classifier becomes linear in logarithmic-space.
	
	\begin{equation}
		\begin{aligned}
			log \big[ P(X|C_k) \big] &\propto log \bigg( P(C_k)\prod_{i=1}^n p_{ki}^{x_i} \bigg) \\
			&= log \bigg(P(C_k) \bigg) + \sum_{i=1}^n x_i \cdot log(p_{ki}) \\
			&= b + \textbf{w}_k \cdot \textbf{x} 
		\end{aligned}
		\label{nb_multinom_log}
	\end{equation}
		
		Additionally, the product must be adjusted for the case when a certain event (appearance of word $ i $) does not occur. Since non-occurrence will be denoted as a frequency equal to zero, such values must be smoothed out to prevent them from nullifying the entire equation.
	
	\subparagraph{Bernoulli}
		This variant assumes multivariate Bernoulli distributions for the features. Particularly, each feature can be represented with a boolean variable. Hence, the classifier requires the feature sets to be passed in the form of vectors composed of binary variables each representing a feature. The classification is based on the decision rule in [\ref{nb_bernoulli}], for class $ C_k $ and feature $ x_i $. The probability of class $ C_k $ producing the feature $ x_i $ is denoted by $ p_{ki} $. We can see that the non-occurrence problem from the Multinomial variant is being explicitly addressed here by penalizing for non-occurrence of a given feature $ x_i $.
	
	\begin{equation}
		P \big(\textbf{x}|C_k \big) = \prod p_{ki}^{x_i}(1-p_{ki})^{(1-x_i)}
		\label{nb_bernoulli}
	\end{equation}